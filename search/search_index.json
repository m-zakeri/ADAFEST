{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ADAFEST: A data-driven apparatus for estimating software testability Morteza Zakeri \u2020 \u2020 Ph.D. Student, Iran University of Science and Technology , Tehran, Iran (m-zakeri@live.com). Version 1.0.0 (18 August 2022) \u251c Download [PDF] version Abstract\u2014 C onnecting runtime information to the static properties of the program is a key point in measuring software quality, including testability. Despite a large number of researches on software testability, we observed that the relationship between testability and test adequacy criteria had not been studied, and testability metrics still are far from measuring the actual test effectiveness and effort. We hypothesize that testability has a significant impact on automatic testing tools. Therefore, we propose a new methodology to measure and quantify software testability by exploiting both runtime information and static properties of the source code. Index Terms: Software testability, software metrics, code coverage, machine learning. This page provides supplementary materials for ADAFEST project, including a full list of source code metrics, a full list of case studies, and description of datasets. Appendix A: source code metrics The list of all metrics along with their quality subject, full name, and granularity used in our experimental study. Subject Metric abbreviation Metric full name Granularity Size/Count CSLOC Class line of code Class CSNOST Class number of statements Class CSNOSM Class number of static methods Class CSNOSA Class number of static attributes Class CSNOIM Class number of instance methods Class CSNOIA Class number of instance attributes Class CSNOM Class number of methods Class CSNOMNAMM Class number of not accessor or mutator methods Class CSNOCON Class number of constructors Class CSNOP Class number of parameters Class PKLOC Package line of code Package PKNOST Package number of statements Package PKNOSM Package number of static methods Package PKNOSA Package number of static attributes Package PKNOIM Package number of instance methods Package PKNOIA Package number of instance attributes Package PKNOMNAMM Package number of not accessor or mutator methods Package PKNOCS Package number of classes Package PKNOFL Package number of files Package Complexity CSCC Class cyclomatic complexity Class CSNESTING Class nesting level of control constructs Class CSPATH Class number of unique paths across a body of code Class CSKNOTS Measure of overlapping jumps Class PKCC Package cyclomatic complexity Package PKNESTING Package nesting level of control constructs Package Cohesion LOCM Lack of Cohesion in Methods Class Coupling CBO Coupling between objects Class RFC Response for a class Class FANIN Total numbers of inputs a class functions uses plus the number of unique subprograms calling class functions. Class FANOUT Total of functions calls plus parameters set/modify of class functions Class DEPENDS All dependencies of the class Class DEPENDSBY Entities depended on by the class Class CFNAMM Called foreign not accessor or mutator methods Class ATFD Access to foreign data Class DAC Data abstraction coupling Class NOMCALL Number of method calls Class Visibility CSNODM Class number of default methods Class CSNOPM Class number of private methods Class CSNOPRM Class number of protected methods Class CSNOPLM Class number of public methods Class CSNOAM Class number of accessor methods Class PKNODM Package number of default methods Package PKNOPM Package number of private methods Package PKNOPRM Package number of protected methods Package PKNOPLM Package number of public methods Package PKNOAM Package number of accessor methods Package Inheritance DIT Depth of inheritance tree Class NOC Number of children Class NOP Number of parents Class NIM Number of inherited methods Class NMO Number of methods overridden Class NOII Number of implemented interfaces Class PKNOI Package number of interfaces Package PKNOAC Package number of abstract classes Package Appendix B: benchmark projects List of Java projects in SF110 corpus Project Domain Java files Line of codes Jgaap - 17 1009 Netweaver - 204 24380 Squirrel-sql - 1151 122959 Sweethome3d - 185 68704 Vuze - 3304 517207 Freemind - 472 62702 Checkstyle - 169 19575 Weka - 1031 243941 Liferay - 8345 1553460 Pdfsam - 369 35922 Water-simulator - 49 6503 Firebird - 258 40005 Imsmart - 20 1039 Dsachat - 32 3861 Jdbacl - 126 18434 Omjstate - 23 593 Beanbin - 88 3788 Templatedetails - 1 391 Inspirento - 36 2427 Jsecurity - 298 13134 Jmca - 25 14885 Tullibee - 20 3236 Nekomud - 10 363 Geo-google - 62 6623 Byuic - 12 5874 Jwbf - 69 5848 Saxpath - 16 2620 Jni-inchi - 24 1156 Jipa - 3 392 Gangup - 95 11088 Greencow - 1 6 Apbsmem - 50 4406 A4j - 45 3602 Bpmail - 37 1681 Xisemele - 56 1805 Httpanalyzer - 19 3588 Javaviewcontrol - 17 4618 Sbmlreader2 - 6 499 Corina - 349 41290 Schemaspy - 72 10008 Petsoar - 76 2255 Javabullboard - 44 8297 Diffi - 10 524 Gaj - 14 320 Glengineer - 41 3124 Follow - 60 4813 Asphodel - 24 691 Lilith - 295 46198 Summa - 584 69341 Lotus - 54 1028 Nutzenportfolio - 84 8268 Dvd-homevideo - 9 2913 Resources4j - 14 1242 Diebierse - 20 1888 Rif - 15 953 Biff - 3 2097 Jiprof - 113 13911 Lagoon - 81 9956 Shp2kml - 4 266 Db-everywhere - 104 7125 Lavalamp - 54 1474 Jhandballmoves - 73 5345 Hft-bomberman - 135 8386 Fps370 - 8 1506 Mygrid - 37 3317 Templateit - 19 2463 Sugar - 37 3147 Noen - 408 18867 Dom4j - 173 18209 Objectexplorer - 88 6988 Jtailgui - 44 2020 Gsftp - 17 2332 Gae-app-manager - 8 411 Biblestudy - 21 2312 Lhamacaw - 108 22772 Jnfe - 68 2096 Echodep - 81 15722 Ext4j - 45 1892 Battlecry - 11 2524 Fim1 - 70 10294 Fixsuite - 25 2665 Openhre - 135 8355 Dash-framework - 22 241 Io-project - 19 698 Caloriecount - 684 61547 Twfbplayer - 104 7240 Sfmis - 19 1288 Wheelwebtool - 113 16275 Javathena - 53 10493 Ipcalculator - 10 2684 Xbus - 203 23514 Ifx-framework - 4027 120355 Shop - 34 3894 At-robots2-j - 231 9459 Jaw-br - 30 4851 Jopenchart - 48 3996 Jiggler - 184 20072 Gfarcegestionfa - 50 3662 Dcparseargs - 6 204 Classviewer - 7 1467 Jcvi-javacommon - 619 45496 Quickserver - 152 16040 Jclo - 3 387 Celwars2009 - 11 2876 Heal - 184 22521 Feudalismgame - 36 3515 Trans-locator - 5 357 Newzgrabber - 39 5874 Falselight - 8 372 Appendix C: dataset The description of DS1 to DS5. Release date The full version of source code will be available as soon as the relevant paper(s) are published.","title":"Home"},{"location":"#adafest-a-data-driven-apparatus-for-estimating-software-testability","text":"Morteza Zakeri \u2020 \u2020 Ph.D. Student, Iran University of Science and Technology , Tehran, Iran (m-zakeri@live.com). Version 1.0.0 (18 August 2022) \u251c Download [PDF] version Abstract\u2014 C onnecting runtime information to the static properties of the program is a key point in measuring software quality, including testability. Despite a large number of researches on software testability, we observed that the relationship between testability and test adequacy criteria had not been studied, and testability metrics still are far from measuring the actual test effectiveness and effort. We hypothesize that testability has a significant impact on automatic testing tools. Therefore, we propose a new methodology to measure and quantify software testability by exploiting both runtime information and static properties of the source code. Index Terms: Software testability, software metrics, code coverage, machine learning. This page provides supplementary materials for ADAFEST project, including a full list of source code metrics, a full list of case studies, and description of datasets.","title":"ADAFEST: A data-driven apparatus for estimating software testability"},{"location":"#appendix-a-source-code-metrics","text":"The list of all metrics along with their quality subject, full name, and granularity used in our experimental study. Subject Metric abbreviation Metric full name Granularity Size/Count CSLOC Class line of code Class CSNOST Class number of statements Class CSNOSM Class number of static methods Class CSNOSA Class number of static attributes Class CSNOIM Class number of instance methods Class CSNOIA Class number of instance attributes Class CSNOM Class number of methods Class CSNOMNAMM Class number of not accessor or mutator methods Class CSNOCON Class number of constructors Class CSNOP Class number of parameters Class PKLOC Package line of code Package PKNOST Package number of statements Package PKNOSM Package number of static methods Package PKNOSA Package number of static attributes Package PKNOIM Package number of instance methods Package PKNOIA Package number of instance attributes Package PKNOMNAMM Package number of not accessor or mutator methods Package PKNOCS Package number of classes Package PKNOFL Package number of files Package Complexity CSCC Class cyclomatic complexity Class CSNESTING Class nesting level of control constructs Class CSPATH Class number of unique paths across a body of code Class CSKNOTS Measure of overlapping jumps Class PKCC Package cyclomatic complexity Package PKNESTING Package nesting level of control constructs Package Cohesion LOCM Lack of Cohesion in Methods Class Coupling CBO Coupling between objects Class RFC Response for a class Class FANIN Total numbers of inputs a class functions uses plus the number of unique subprograms calling class functions. Class FANOUT Total of functions calls plus parameters set/modify of class functions Class DEPENDS All dependencies of the class Class DEPENDSBY Entities depended on by the class Class CFNAMM Called foreign not accessor or mutator methods Class ATFD Access to foreign data Class DAC Data abstraction coupling Class NOMCALL Number of method calls Class Visibility CSNODM Class number of default methods Class CSNOPM Class number of private methods Class CSNOPRM Class number of protected methods Class CSNOPLM Class number of public methods Class CSNOAM Class number of accessor methods Class PKNODM Package number of default methods Package PKNOPM Package number of private methods Package PKNOPRM Package number of protected methods Package PKNOPLM Package number of public methods Package PKNOAM Package number of accessor methods Package Inheritance DIT Depth of inheritance tree Class NOC Number of children Class NOP Number of parents Class NIM Number of inherited methods Class NMO Number of methods overridden Class NOII Number of implemented interfaces Class PKNOI Package number of interfaces Package PKNOAC Package number of abstract classes Package","title":"Appendix A: source code metrics"},{"location":"#appendix-b-benchmark-projects","text":"List of Java projects in SF110 corpus Project Domain Java files Line of codes Jgaap - 17 1009 Netweaver - 204 24380 Squirrel-sql - 1151 122959 Sweethome3d - 185 68704 Vuze - 3304 517207 Freemind - 472 62702 Checkstyle - 169 19575 Weka - 1031 243941 Liferay - 8345 1553460 Pdfsam - 369 35922 Water-simulator - 49 6503 Firebird - 258 40005 Imsmart - 20 1039 Dsachat - 32 3861 Jdbacl - 126 18434 Omjstate - 23 593 Beanbin - 88 3788 Templatedetails - 1 391 Inspirento - 36 2427 Jsecurity - 298 13134 Jmca - 25 14885 Tullibee - 20 3236 Nekomud - 10 363 Geo-google - 62 6623 Byuic - 12 5874 Jwbf - 69 5848 Saxpath - 16 2620 Jni-inchi - 24 1156 Jipa - 3 392 Gangup - 95 11088 Greencow - 1 6 Apbsmem - 50 4406 A4j - 45 3602 Bpmail - 37 1681 Xisemele - 56 1805 Httpanalyzer - 19 3588 Javaviewcontrol - 17 4618 Sbmlreader2 - 6 499 Corina - 349 41290 Schemaspy - 72 10008 Petsoar - 76 2255 Javabullboard - 44 8297 Diffi - 10 524 Gaj - 14 320 Glengineer - 41 3124 Follow - 60 4813 Asphodel - 24 691 Lilith - 295 46198 Summa - 584 69341 Lotus - 54 1028 Nutzenportfolio - 84 8268 Dvd-homevideo - 9 2913 Resources4j - 14 1242 Diebierse - 20 1888 Rif - 15 953 Biff - 3 2097 Jiprof - 113 13911 Lagoon - 81 9956 Shp2kml - 4 266 Db-everywhere - 104 7125 Lavalamp - 54 1474 Jhandballmoves - 73 5345 Hft-bomberman - 135 8386 Fps370 - 8 1506 Mygrid - 37 3317 Templateit - 19 2463 Sugar - 37 3147 Noen - 408 18867 Dom4j - 173 18209 Objectexplorer - 88 6988 Jtailgui - 44 2020 Gsftp - 17 2332 Gae-app-manager - 8 411 Biblestudy - 21 2312 Lhamacaw - 108 22772 Jnfe - 68 2096 Echodep - 81 15722 Ext4j - 45 1892 Battlecry - 11 2524 Fim1 - 70 10294 Fixsuite - 25 2665 Openhre - 135 8355 Dash-framework - 22 241 Io-project - 19 698 Caloriecount - 684 61547 Twfbplayer - 104 7240 Sfmis - 19 1288 Wheelwebtool - 113 16275 Javathena - 53 10493 Ipcalculator - 10 2684 Xbus - 203 23514 Ifx-framework - 4027 120355 Shop - 34 3894 At-robots2-j - 231 9459 Jaw-br - 30 4851 Jopenchart - 48 3996 Jiggler - 184 20072 Gfarcegestionfa - 50 3662 Dcparseargs - 6 204 Classviewer - 7 1467 Jcvi-javacommon - 619 45496 Quickserver - 152 16040 Jclo - 3 387 Celwars2009 - 11 2876 Heal - 184 22521 Feudalismgame - 36 3515 Trans-locator - 5 357 Newzgrabber - 39 5874 Falselight - 8 372","title":"Appendix B: benchmark projects"},{"location":"#appendix-c-dataset","text":"The description of DS1 to DS5.","title":"Appendix C: dataset"},{"location":"#release-date","text":"The full version of source code will be available as soon as the relevant paper(s) are published.","title":"Release date"},{"location":"benchmarks/","text":"ADAFEST benchmarks Appendix B: benchmark projects List of Java projects in SF110 corpus Project Domain Java files Line of codes Jgaap - 17 1009 Netweaver - 204 24380 Squirrel-sql - 1151 122959 Sweethome3d - 185 68704 Vuze - 3304 517207 Freemind - 472 62702 Checkstyle - 169 19575 Weka - 1031 243941 Liferay - 8345 1553460 Pdfsam - 369 35922 Water-simulator - 49 6503 Firebird - 258 40005 Imsmart - 20 1039 Dsachat - 32 3861 Jdbacl - 126 18434 Omjstate - 23 593 Beanbin - 88 3788 Templatedetails - 1 391 Inspirento - 36 2427 Jsecurity - 298 13134 Jmca - 25 14885 Tullibee - 20 3236 Nekomud - 10 363 Geo-google - 62 6623 Byuic - 12 5874 Jwbf - 69 5848 Saxpath - 16 2620 Jni-inchi - 24 1156 Jipa - 3 392 Gangup - 95 11088 Greencow - 1 6 Apbsmem - 50 4406 A4j - 45 3602 Bpmail - 37 1681 Xisemele - 56 1805 Httpanalyzer - 19 3588 Javaviewcontrol - 17 4618 Sbmlreader2 - 6 499 Corina - 349 41290 Schemaspy - 72 10008 Petsoar - 76 2255 Javabullboard - 44 8297 Diffi - 10 524 Gaj - 14 320 Glengineer - 41 3124 Follow - 60 4813 Asphodel - 24 691 Lilith - 295 46198 Summa - 584 69341 Lotus - 54 1028 Nutzenportfolio - 84 8268 Dvd-homevideo - 9 2913 Resources4j - 14 1242 Diebierse - 20 1888 Rif - 15 953 Biff - 3 2097 Jiprof - 113 13911 Lagoon - 81 9956 Shp2kml - 4 266 Db-everywhere - 104 7125 Lavalamp - 54 1474 Jhandballmoves - 73 5345 Hft-bomberman - 135 8386 Fps370 - 8 1506 Mygrid - 37 3317 Templateit - 19 2463 Sugar - 37 3147 Noen - 408 18867 Dom4j - 173 18209 Objectexplorer - 88 6988 Jtailgui - 44 2020 Gsftp - 17 2332 Gae-app-manager - 8 411 Biblestudy - 21 2312 Lhamacaw - 108 22772 Jnfe - 68 2096 Echodep - 81 15722 Ext4j - 45 1892 Battlecry - 11 2524 Fim1 - 70 10294 Fixsuite - 25 2665 Openhre - 135 8355 Dash-framework - 22 241 Io-project - 19 698 Caloriecount - 684 61547 Twfbplayer - 104 7240 Sfmis - 19 1288 Wheelwebtool - 113 16275 Javathena - 53 10493 Ipcalculator - 10 2684 Xbus - 203 23514 Ifx-framework - 4027 120355 Shop - 34 3894 At-robots2-j - 231 9459 Jaw-br - 30 4851 Jopenchart - 48 3996 Jiggler - 184 20072 Gfarcegestionfa - 50 3662 Dcparseargs - 6 204 Classviewer - 7 1467 Jcvi-javacommon - 619 45496 Quickserver - 152 16040 Jclo - 3 387 Celwars2009 - 11 2876 Heal - 184 22521 Feudalismgame - 36 3515 Trans-locator - 5 357 Newzgrabber - 39 5874 Falselight - 8 372","title":"Benchmarks"},{"location":"benchmarks/#adafest-benchmarks","text":"","title":"ADAFEST benchmarks"},{"location":"benchmarks/#appendix-b-benchmark-projects","text":"List of Java projects in SF110 corpus Project Domain Java files Line of codes Jgaap - 17 1009 Netweaver - 204 24380 Squirrel-sql - 1151 122959 Sweethome3d - 185 68704 Vuze - 3304 517207 Freemind - 472 62702 Checkstyle - 169 19575 Weka - 1031 243941 Liferay - 8345 1553460 Pdfsam - 369 35922 Water-simulator - 49 6503 Firebird - 258 40005 Imsmart - 20 1039 Dsachat - 32 3861 Jdbacl - 126 18434 Omjstate - 23 593 Beanbin - 88 3788 Templatedetails - 1 391 Inspirento - 36 2427 Jsecurity - 298 13134 Jmca - 25 14885 Tullibee - 20 3236 Nekomud - 10 363 Geo-google - 62 6623 Byuic - 12 5874 Jwbf - 69 5848 Saxpath - 16 2620 Jni-inchi - 24 1156 Jipa - 3 392 Gangup - 95 11088 Greencow - 1 6 Apbsmem - 50 4406 A4j - 45 3602 Bpmail - 37 1681 Xisemele - 56 1805 Httpanalyzer - 19 3588 Javaviewcontrol - 17 4618 Sbmlreader2 - 6 499 Corina - 349 41290 Schemaspy - 72 10008 Petsoar - 76 2255 Javabullboard - 44 8297 Diffi - 10 524 Gaj - 14 320 Glengineer - 41 3124 Follow - 60 4813 Asphodel - 24 691 Lilith - 295 46198 Summa - 584 69341 Lotus - 54 1028 Nutzenportfolio - 84 8268 Dvd-homevideo - 9 2913 Resources4j - 14 1242 Diebierse - 20 1888 Rif - 15 953 Biff - 3 2097 Jiprof - 113 13911 Lagoon - 81 9956 Shp2kml - 4 266 Db-everywhere - 104 7125 Lavalamp - 54 1474 Jhandballmoves - 73 5345 Hft-bomberman - 135 8386 Fps370 - 8 1506 Mygrid - 37 3317 Templateit - 19 2463 Sugar - 37 3147 Noen - 408 18867 Dom4j - 173 18209 Objectexplorer - 88 6988 Jtailgui - 44 2020 Gsftp - 17 2332 Gae-app-manager - 8 411 Biblestudy - 21 2312 Lhamacaw - 108 22772 Jnfe - 68 2096 Echodep - 81 15722 Ext4j - 45 1892 Battlecry - 11 2524 Fim1 - 70 10294 Fixsuite - 25 2665 Openhre - 135 8355 Dash-framework - 22 241 Io-project - 19 698 Caloriecount - 684 61547 Twfbplayer - 104 7240 Sfmis - 19 1288 Wheelwebtool - 113 16275 Javathena - 53 10493 Ipcalculator - 10 2684 Xbus - 203 23514 Ifx-framework - 4027 120355 Shop - 34 3894 At-robots2-j - 231 9459 Jaw-br - 30 4851 Jopenchart - 48 3996 Jiggler - 184 20072 Gfarcegestionfa - 50 3662 Dcparseargs - 6 204 Classviewer - 7 1467 Jcvi-javacommon - 619 45496 Quickserver - 152 16040 Jclo - 3 387 Celwars2009 - 11 2876 Heal - 184 22521 Feudalismgame - 36 3515 Trans-locator - 5 357 Newzgrabber - 39 5874 Falselight - 8 372","title":"Appendix B: benchmark projects"},{"location":"datasets/","text":"ADAFEST datasets The ADAFEST datasets contains all experimental data used in our machine learning pipeline at ADAFEST project. Our testability prediction dataset consists of several CSV files which differ in preprocessing steps used to generate them. Each row denotes metrics for a Java class. Each column is a source code metrics or test metrics obtained by running EvoSuite on the corresponding class under test. The first column is a long name (package_name.class_name) of a Java class. More information will be available in ADAFEST publications . All data available on Zenodo: The current version of the testability prediction dataset (dataset06\u2014version 0.6.x) contains the following files: DS060Raw.csv : Contains only source code metrics for 19,720 Java classes. The last column indicates the number of Java classes in the enclosing file of the presented Java class. Actually, SF110 contains more than 23K Java classes. We removed small projects and projects that most of their classes are data classes used as database models. DS060RawLabeled.csv : The same DS060Raw.csv with ten attached columns containing dynamically computed metrics obtained by running EvoSuite test data generation tools. The last four columns are combinatory metrics computed based on the primary metrics given by EvoSuite. The most useful metrics are statement coverage, branch coverage, and the number of generated tests. More details of EvoSuite configuration and runtime metrics are available in ADAFEST relevant papers. DS06010.csv and DS06011.csv : This file contains 18,324 Java classes. Irrelevant samples in DS060RawLabeled.csv (i.e., simple classes, data class, files with more than one class, classed with zero number of test cases) have been removed in this file. DS06012.csv : Class with outlier metrics have been deleted from DS06011.csv , and this file contains 16,165 Java classes. DS06012_outliers_only.csv : This file contains Java classes detected as an outlier by the local outlier factor (LOF) algorithm. DS06310.csv : Package metrics (used as context vector in our testability prediction approach) have been removed from DS06012.csv in this file. DS06410.csv : Package metrics (used as context vector in our testability prediction approach) and lexical metrics have been removed from DS06012.csv in this file. DS06510.csv : Sub-metrics (systematically generated metrics) have been removed from DS06012.csv in this file.","title":"Datasets"},{"location":"datasets/#adafest-datasets","text":"The ADAFEST datasets contains all experimental data used in our machine learning pipeline at ADAFEST project. Our testability prediction dataset consists of several CSV files which differ in preprocessing steps used to generate them. Each row denotes metrics for a Java class. Each column is a source code metrics or test metrics obtained by running EvoSuite on the corresponding class under test. The first column is a long name (package_name.class_name) of a Java class. More information will be available in ADAFEST publications . All data available on Zenodo: The current version of the testability prediction dataset (dataset06\u2014version 0.6.x) contains the following files: DS060Raw.csv : Contains only source code metrics for 19,720 Java classes. The last column indicates the number of Java classes in the enclosing file of the presented Java class. Actually, SF110 contains more than 23K Java classes. We removed small projects and projects that most of their classes are data classes used as database models. DS060RawLabeled.csv : The same DS060Raw.csv with ten attached columns containing dynamically computed metrics obtained by running EvoSuite test data generation tools. The last four columns are combinatory metrics computed based on the primary metrics given by EvoSuite. The most useful metrics are statement coverage, branch coverage, and the number of generated tests. More details of EvoSuite configuration and runtime metrics are available in ADAFEST relevant papers. DS06010.csv and DS06011.csv : This file contains 18,324 Java classes. Irrelevant samples in DS060RawLabeled.csv (i.e., simple classes, data class, files with more than one class, classed with zero number of test cases) have been removed in this file. DS06012.csv : Class with outlier metrics have been deleted from DS06011.csv , and this file contains 16,165 Java classes. DS06012_outliers_only.csv : This file contains Java classes detected as an outlier by the local outlier factor (LOF) algorithm. DS06310.csv : Package metrics (used as context vector in our testability prediction approach) have been removed from DS06012.csv in this file. DS06410.csv : Package metrics (used as context vector in our testability prediction approach) and lexical metrics have been removed from DS06012.csv in this file. DS06510.csv : Sub-metrics (systematically generated metrics) have been removed from DS06012.csv in this file.","title":"ADAFEST datasets"},{"location":"metrics/","text":"ADAFEST metrics Appendix A: source code metrics The list of all metrics along with their quality subject, full name, and granularity used in our experimental study. Subject Metric abbreviation Metric full name Granularity Size/Count CSLOC Class line of code Class CSNOST Class number of statements Class CSNOSM Class number of static methods Class CSNOSA Class number of static attributes Class CSNOIM Class number of instance methods Class CSNOIA Class number of instance attributes Class CSNOM Class number of methods Class CSNOMNAMM Class number of not accessor or mutator methods Class CSNOCON Class number of constructors Class CSNOP Class number of parameters Class PKLOC Package line of code Package PKNOST Package number of statements Package PKNOSM Package number of static methods Package PKNOSA Package number of static attributes Package PKNOIM Package number of instance methods Package PKNOIA Package number of instance attributes Package PKNOMNAMM Package number of not accessor or mutator methods Package PKNOCS Package number of classes Package PKNOFL Package number of files Package Complexity CSCC Class cyclomatic complexity Class CSNESTING Class nesting level of control constructs Class CSPATH Class number of unique paths across a body of code Class CSKNOTS Measure of overlapping jumps Class PKCC Package cyclomatic complexity Package PKNESTING Package nesting level of control constructs Package Cohesion LOCM Lack of Cohesion in Methods Class Coupling CBO Coupling between objects Class RFC Response for a class Class FANIN Total numbers of inputs a class functions uses plus the number of unique subprograms calling class functions. Class FANOUT Total of functions calls plus parameters set/modify of class functions Class DEPENDS All dependencies of the class Class DEPENDSBY Entities depended on by the class Class CFNAMM Called foreign not accessor or mutator methods Class ATFD Access to foreign data Class DAC Data abstraction coupling Class NOMCALL Number of method calls Class Visibility CSNODM Class number of default methods Class CSNOPM Class number of private methods Class CSNOPRM Class number of protected methods Class CSNOPLM Class number of public methods Class CSNOAM Class number of accessor methods Class PKNODM Package number of default methods Package PKNOPM Package number of private methods Package PKNOPRM Package number of protected methods Package PKNOPLM Package number of public methods Package PKNOAM Package number of accessor methods Package Inheritance DIT Depth of inheritance tree Class NOC Number of children Class NOP Number of parents Class NIM Number of inherited methods Class NMO Number of methods overridden Class NOII Number of implemented interfaces Class PKNOI Package number of interfaces Package PKNOAC Package number of abstract classes Package","title":"Metrics"},{"location":"metrics/#adafest-metrics","text":"","title":"ADAFEST metrics"},{"location":"metrics/#appendix-a-source-code-metrics","text":"The list of all metrics along with their quality subject, full name, and granularity used in our experimental study. Subject Metric abbreviation Metric full name Granularity Size/Count CSLOC Class line of code Class CSNOST Class number of statements Class CSNOSM Class number of static methods Class CSNOSA Class number of static attributes Class CSNOIM Class number of instance methods Class CSNOIA Class number of instance attributes Class CSNOM Class number of methods Class CSNOMNAMM Class number of not accessor or mutator methods Class CSNOCON Class number of constructors Class CSNOP Class number of parameters Class PKLOC Package line of code Package PKNOST Package number of statements Package PKNOSM Package number of static methods Package PKNOSA Package number of static attributes Package PKNOIM Package number of instance methods Package PKNOIA Package number of instance attributes Package PKNOMNAMM Package number of not accessor or mutator methods Package PKNOCS Package number of classes Package PKNOFL Package number of files Package Complexity CSCC Class cyclomatic complexity Class CSNESTING Class nesting level of control constructs Class CSPATH Class number of unique paths across a body of code Class CSKNOTS Measure of overlapping jumps Class PKCC Package cyclomatic complexity Package PKNESTING Package nesting level of control constructs Package Cohesion LOCM Lack of Cohesion in Methods Class Coupling CBO Coupling between objects Class RFC Response for a class Class FANIN Total numbers of inputs a class functions uses plus the number of unique subprograms calling class functions. Class FANOUT Total of functions calls plus parameters set/modify of class functions Class DEPENDS All dependencies of the class Class DEPENDSBY Entities depended on by the class Class CFNAMM Called foreign not accessor or mutator methods Class ATFD Access to foreign data Class DAC Data abstraction coupling Class NOMCALL Number of method calls Class Visibility CSNODM Class number of default methods Class CSNOPM Class number of private methods Class CSNOPRM Class number of protected methods Class CSNOPLM Class number of public methods Class CSNOAM Class number of accessor methods Class PKNODM Package number of default methods Package PKNOPM Package number of private methods Package PKNOPRM Package number of protected methods Package PKNOPLM Package number of public methods Package PKNOAM Package number of accessor methods Package Inheritance DIT Depth of inheritance tree Class NOC Number of children Class NOP Number of parents Class NIM Number of inherited methods Class NMO Number of methods overridden Class NOII Number of implemented interfaces Class PKNOI Package number of interfaces Package PKNOAC Package number of abstract classes Package","title":"Appendix A: source code metrics"},{"location":"publications/","text":"ADAFEST publications [1] Zakeri-Nasrabadi, M., & Parsa, S. (2021). Learning to predict software testability. 2021 26th International Computer Conference, Computer Society of Iran (CSICC), 1\u20135. https://doi.org/10.1109/CSICC52343.2021.9420548 [2] Zakeri\u2010Nasrabadi, M., & Parsa, S. (2021). Learning to predict test effectiveness. International Journal of Intelligent Systems. https://doi.org/10.1002/int.22722 [3] Zakeri\u2010Nasrabadi, M., & Parsa, S. (2022). An ensemble meta-estimator to predict source code testability. Applied Soft Computing.","title":"Publications"},{"location":"publications/#adafest-publications","text":"[1] Zakeri-Nasrabadi, M., & Parsa, S. (2021). Learning to predict software testability. 2021 26th International Computer Conference, Computer Society of Iran (CSICC), 1\u20135. https://doi.org/10.1109/CSICC52343.2021.9420548 [2] Zakeri\u2010Nasrabadi, M., & Parsa, S. (2021). Learning to predict test effectiveness. International Journal of Intelligent Systems. https://doi.org/10.1002/int.22722 [3] Zakeri\u2010Nasrabadi, M., & Parsa, S. (2022). An ensemble meta-estimator to predict source code testability. Applied Soft Computing.","title":"ADAFEST publications"},{"location":"related_projects/","text":"Related projects ADAFEST is part of Morteza ZAKERI's Ph.D. works. The relevant projects in this series are: ARTA : requirement testability measurement tool CodART : source code testability improvement tool QualityMeter : software quality measurement toolkit OpenUnderstand : Program symbol table generator framework [STaRT]: to be announced.","title":"Related projects"},{"location":"related_projects/#related-projects","text":"ADAFEST is part of Morteza ZAKERI's Ph.D. works. The relevant projects in this series are: ARTA : requirement testability measurement tool CodART : source code testability improvement tool QualityMeter : software quality measurement toolkit OpenUnderstand : Program symbol table generator framework [STaRT]: to be announced.","title":"Related projects"},{"location":"modules/coverageability_prediction/","text":"Coverageability prediction The replication package for the paper 'Learning to predict test effectiveness' published in International Journal of Intelligent Systems. Goal This script implements machine learning models for predicting the expected value of statement and branch coverage presented in International Journal of Intelligent Systems. Machine learning models Model 1: DecisionTreeRegressor Model 2: RandomForestRegressor Model 3: GradientBoostingRegressor Model 4: HistGradientBoostingRegressor Model 5: SGDRegressor Model 6: MLPRegressor Learning datasets Dataset Applied preprocessing Number of metrics DS1: (default) Simple classes elimination, data classes elimination, outliers elimination, and metric standardization 262 DS2: DS1 + Feature selection 20 DS3: DS1 + Context vector elimination 194 DS4: DS1 + Context vector elimination and lexical metrics elimination 177 DS5: DS1 + Systematically generated metrics elimination 71 DS6: Top 15 important source code metrics affecting Coverageability Model dependent variable E[C] = (1/2 Statement coverage + 1/2 Branch coverage) * b/|n| Results The results will be saved in sklearn_models6c Inferences Use the method inference_model2 of the class Regression to predict testability of new Java classes MultioutputClassification https://scikit-learn.org/stable/modules/multiclass.html#multioutput-classification Multioutput-multiclass classification (also known as multitask classification) Source code in adafest\\code\\testability\\ml_models_coverageability.py class MultioutputClassification : \"\"\" https://scikit-learn.org/stable/modules/multiclass.html#multioutput-classification Multioutput-multiclass classification (also known as multitask classification) \"\"\" pass Regression Source code in adafest\\code\\testability\\ml_models_coverageability.py class Regression ( object ): def __init__ ( self , df_path = r 'dataset06/DS06013.csv' , avg_type = None ): self . df = pd . read_csv ( df_path , delimiter = ',' , index_col = False ) self . df [ 'Label_Combine1' ] = self . df [ 'Label_Combine1' ] * 0.01 self . df [ 'Label_LineCoverage' ] = self . df [ 'Label_LineCoverage' ] * 0.01 self . df [ 'Label_BranchCoverage' ] = self . df [ 'Label_BranchCoverage' ] * 0.01 self . df [ 'Coverageability1' ] = self . df [ 'Coverageability1' ] * 0.01 label_coverageability = self . df [ 'Label_Combine1' ] / self . df [ 'Tests' ] # (Arithmetic mean) if avg_type is not None : label_coverageability2 = list () # (Geometric mean) label_coverageability3 = list () # (Harmonic mean) for row in self . df . iterrows (): print ( row [ 1 ][ - 3 ]) label_coverageability2 . append ( ( math . sqrt ( row [ 1 ][ - 4 ] * row [ 1 ][ - 5 ])) / row [ 1 ][ - 3 ] ) # (Geometric mean) label_coverageability3 . append ( (( 2 * row [ 1 ][ - 4 ] * row [ 1 ][ - 5 ]) / ( row [ 1 ][ - 4 ] + row [ 1 ][ - 5 ])) / row [ 1 ][ - 3 ] ) # (Harmonic mean) label_coverageability2 = pd . DataFrame ( label_coverageability2 ) label_coverageability3 = pd . DataFrame ( label_coverageability3 ) # print('Before applying filter:', self.df.shape) # self.df = self.df.loc[(self.df.Label_BranchCoverage <= 0.50)] # self.df = self.df.loc[(self.df.Label_LineCoverage <= 0.50)] # print('After applying filter:', self.df.shape) # index -1: Coveragability1 (i.e., Testability) # index -2: E[C] = 1/2 branch * line ==> models names: XXX1_DSX # index -3: Test suite size # index -4: BranchCoverage ==> model names: XXX2_DSX # index -5: LineCoverage ==> model names: XXX3_DSX self . X_train1 , self . X_test1 , self . y_train , self . y_test = train_test_split ( self . df . iloc [:, 1 : - 5 ], # self.df.iloc[:, -2], # label_coverageability, self . df [ 'Label_BranchCoverage' ], test_size = 0.25 , random_state = 42 , # stratify=self.df.iloc[:, -1] ) \"\"\" # --------------------------------------- # -- Feature selection (For DS2) selector = feature_selection.SelectKBest(feature_selection.f_regression, k=15) # clf = linear_model.LassoCV(eps=1e-3, n_alphas=100, normalize=True, max_iter=5000, tol=1e-4) # clf.fit(self.X_train1, self.y_train) # importance = np.abs(clf.coef_) # print('importance', importance) # clf = RandomForestRegressor() # selector = feature_selection.SelectFromModel(clf, prefit=False, norm_order=2, max_features=20, threshold=None) selector.fit(self.X_train1, self.y_train) # Get columns to keep and create new dataframe with only selected features cols = selector.get_support(indices=True) self.X_train1 = self.X_train1.iloc[:, cols] self.X_test1 = self.X_test1.iloc[:, cols] print('Selected columns by feature selection:', self.X_train1.columns) # quit() # -- End of feature selection \"\"\" # --------------------------------------- # Standardization self . scaler = preprocessing . RobustScaler ( with_centering = True , with_scaling = True ) # self.scaler = preprocessing.StandardScaler() self . scaler . fit ( self . X_train1 ) self . X_train = self . scaler . transform ( self . X_train1 ) self . X_test = self . scaler . transform ( self . X_test1 ) dump ( self . scaler , 'DS06510.joblib' ) # quit() def inference_model ( self , model = None , model_path = None ): if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test [ 3 : 4 , ]) print ( 'X_test {0} ' . format ( self . X_test [ 3 : 4 , ])) print ( '------' ) print ( 'y_test or y_true {0} ' . format ( y_true [ 3 : 4 , ])) print ( '------' ) print ( 'y_pred by model {0} ' . format ( y_pred )) y_true , y_pred = self . y_test , model . predict ( self . X_test ) df_new = pd . DataFrame ( columns = self . df . columns ) for i , row in self . y_test . iteritems (): print ( '' , i , row ) df_new = df_new . append ( self . df . loc [ i ], ignore_index = True ) df_new [ 'y_true' ] = self . y_test . values df_new [ 'y_pred' ] = list ( y_pred ) df_new . to_csv ( model_path [: - 7 ] + '_inference_result.csv' , index = True , index_label = 'Row' ) def inference_model2 ( self , model = None , model_path = None , predict_data_path = None ): if model is None : model = joblib . load ( model_path ) df_predict_data = pd . read_csv ( predict_data_path , delimiter = ',' , index_col = False ) X_test1 = df_predict_data . iloc [:, 1 :] X_test = self . scaler . transform ( X_test1 ) y_pred = model . predict ( X_test ) df_new = pd . DataFrame ( df_predict_data . iloc [:, 0 ], columns = [ 'Class' ]) df_new [ 'PredictedTestability' ] = list ( y_pred ) print ( df_new ) # df_new.to_csv(r'dataset06/refactored01010_predicted_testability.csv', index=True, index_label='Row') def evaluate_model ( self , model = None , model_path = None ): # X = self.data_frame.iloc[:, 1:-4] # y = self.data_frame.iloc[:, -4] # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0) if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test ) # y_score = model.predict_proba(X_test) # Print all classifier model metrics print ( 'Evaluating regressor ...' ) print ( 'Regressor minimum prediction' , min ( y_pred ), 'Regressor maximum prediction' , max ( y_pred )) df = pd . DataFrame () df [ 'r2_score_uniform_average' ] = [ r2_score ( y_true , y_pred , multioutput = 'uniform_average' )] df [ 'r2_score_variance_weighted' ] = [ r2_score ( y_true , y_pred , multioutput = 'variance_weighted' )] df [ 'explained_variance_score_uniform_average' ] = [ explained_variance_score ( y_true , y_pred , multioutput = 'uniform_average' )] df [ 'explained_variance_score_variance_weighted' ] = [ explained_variance_score ( y_true , y_pred , multioutput = 'variance_weighted' )] df [ 'mean_absolute_error' ] = [ mean_absolute_error ( y_true , y_pred )] df [ 'mean_squared_error_MSE' ] = [ mean_squared_error ( y_true , y_pred )] df [ 'mean_squared_error_RMSE' ] = [ mean_squared_error ( y_true , y_pred , squared = False )] df [ 'median_absolute_error' ] = [ median_absolute_error ( y_true , y_pred )] if min ( y_pred ) >= 0 : df [ 'mean_squared_log_error' ] = [ mean_squared_log_error ( y_true , y_pred )] # To handl ValueError: Mean Tweedie deviance error with power=2 # can only be used on strictly positive y and y_pred. if min ( y_pred > 0 ) and min ( y_true ) > 0 : df [ 'mean_poisson_deviance' ] = [ mean_poisson_deviance ( y_true , y_pred , )] df [ 'mean_gamma_deviance' ] = [ mean_gamma_deviance ( y_true , y_pred , )] df [ 'max_error' ] = [ max_error ( y_true , y_pred )] df . to_csv ( model_path [: - 7 ] + '_evaluation_metrics_R1.csv' , index = True , index_label = 'Row' ) def evaluate_model_class ( self , model = None , model_path = None ): if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test ) df_new = pd . DataFrame ( y_true ) df_new [ 'y_pred' ] = y_pred testability_labels = [ 'VeryLow' , 'Low' , 'Moderate' , 'High' , 'VeryHigh' ] testability_labels = [ 'Low' , 'Moderate' , 'High' ] bins = [ - 1.250 , 0.250 , 0.750 , 1.250 ] # bins = 5 df_new [ 'y_ture_nominal' ] = pd . cut ( df_new . loc [:, [ 'Coverageability1' ]] . T . squeeze (), bins = bins , labels = testability_labels , right = True ) df_new [ 'y_pred_nominal' ] = pd . cut ( df_new . loc [:, [ 'y_pred' ]] . T . squeeze (), bins = bins , labels = testability_labels , right = True ) print ( df_new ) # df_new.to_csv('XXXXX.csv') y_true = df_new [ 'y_ture_nominal' ] y_pred = df_new [ 'y_pred_nominal' ] y_score = y_pred # Print all classifier model metrics print ( 'Evaluating classifier ...' ) df = pd . DataFrame () print ( y_pred ) try : df [ 'accuracy_score' ] = [ accuracy_score ( y_true , y_pred )] df [ 'balanced_accuracy_score' ] = [ balanced_accuracy_score ( y_true , y_pred )] df [ 'precision_score_macro' ] = [ precision_score ( y_true , y_pred , average = 'macro' )] df [ 'precision_score_micro' ] = [ precision_score ( y_true , y_pred , average = 'micro' )] df [ 'recall_score_macro' ] = [ recall_score ( y_true , y_pred , average = 'macro' )] df [ 'recall_score_micro' ] = [ recall_score ( y_true , y_pred , average = 'micro' )] df [ 'f1_score_macro' ] = [ f1_score ( y_true , y_pred , average = 'macro' )] df [ 'f1_score_micro' ] = [ f1_score ( y_true , y_pred , average = 'micro' )] df [ 'fbeta_score_macro' ] = [ fbeta_score ( y_true , y_pred , beta = 0.5 , average = 'macro' )] df [ 'fbeta_score_micro' ] = [ fbeta_score ( y_true , y_pred , beta = 0.5 , average = 'micro' )] # df['log_loss'] = [log_loss(y_true, y_score)] # df['roc_auc_score_ovr_macro'] = [roc_auc_score(y_true, y_score, multi_class='ovr', average='macro')] # df['roc_auc_score_ovr_micro'] = [roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')] # df['roc_auc_score_ovo_macro'] = [roc_auc_score(y_true, y_score, multi_class='ovo', average='macro')] # df['roc_auc_score_ovo_micro'] = [roc_auc_score(y_true, y_score, multi_class='ovo', average='weighted')] # print('roc_curve_:', roc_curve(y_true, y_score)) # multiclass format is not supported df . to_csv ( model_path [: - 7 ] + '_evaluation_metrics_C.csv' , index = True , index_label = 'Row' ) except : raise ValueError ( 'The prediction is out of range' ) def regress_with_decision_tree ( self , model_path ): # X = self.data_frame.iloc[:, 1:-4] # y = self.data_frame.iloc[:, -4] # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0) clf = tree . DecisionTreeRegressor () # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.25 , random_state = 42 ) # Set the parameters to be used for tuning by cross-validation parameters = { 'max_depth' : range ( 1 , 100 , 10 ), 'criterion' : [ 'mse' , 'friedman_mse' , 'mae' ], 'min_samples_split' : range ( 2 , 20 , 1 ) } # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # Find the best model using gird-search with cross-validation clf = GridSearchCV ( clf , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 4 , refit = 'neg_root_mean_squared_error' ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # Plots # tree.plot_tree(clf) # plt.show() def regress ( self , model_path : str = None , model_number : int = None ): \"\"\" :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: \"\"\" regressor = None parameters = None if model_number == 1 : regressor = tree . DecisionTreeRegressor ( random_state = 42 , ) # Set the parameters to be used for tuning by cross-validation parameters = { # 'criterion': ['mse', 'friedman_mse', 'mae'], 'max_depth' : range ( 3 , 50 , 5 ), 'min_samples_split' : range ( 2 , 30 , 2 ) } elif model_number == 2 : regressor = RandomForestRegressor ( random_state = 42 , ) parameters = { 'n_estimators' : range ( 100 , 200 , 100 ), # 'criterion': ['mse', 'mae'], 'max_depth' : range ( 10 , 50 , 10 ), # 'min_samples_split': range(2, 30, 2), # 'max_features': ['auto', 'sqrt', 'log2'] } elif model_number == 3 : regressor = GradientBoostingRegressor ( n_estimators = 400 , learning_rate = 0.05 , random_state = 42 , ) parameters = { # 'loss': ['ls', 'lad', ], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_split' : range ( 2 , 30 , 3 ) } elif model_number == 4 : regressor = HistGradientBoostingRegressor ( max_iter = 400 , learning_rate = 0.05 , random_state = 42 , ) parameters = { # 'loss': ['least_squares', 'least_absolute_deviation'], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_leaf' : range ( 5 , 50 , 10 ) } elif model_number == 5 : regressor = linear_model . SGDRegressor ( early_stopping = True , n_iter_no_change = 5 , random_state = 42 , ) parameters = { 'loss' : [ 'squared_loss' , 'huber' , 'epsilon_insensitive' ], 'penalty' : [ 'l2' , 'l1' , 'elasticnet' ], 'max_iter' : range ( 50 , 1000 , 50 ), 'learning_rate' : [ 'invscaling' , 'optimal' , 'constant' , 'adaptive' ], 'eta0' : [ 0.1 , 0.01 ], 'average' : [ 32 , ] } elif model_number == 6 : regressor = MLPRegressor ( random_state = 42 , ) parameters = { 'hidden_layer_sizes' : [( 256 , 100 ), ( 512 , 256 , 100 ), ], 'activation' : [ 'tanh' , ], 'solver' : [ 'adam' , ], 'max_iter' : range ( 50 , 200 , 50 ) } if regressor is None : return if parameters is None : return # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.25 , random_state = 42 ) # Find the best model using gird-search with cross-validation clf = GridSearchCV ( regressor , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 4 , refit = 'neg_root_mean_squared_error' ) print ( 'fitting model number' , model_number ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # self.evaluate_model_class(model=clf, model_path=model_path) # self.inference_model(model=clf, model_path=model_path) print ( '=' * 75 ) def vote ( self , model_path = None , dataset_number = 1 ): # Trained regressors reg1 = load ( r 'sklearn_models6c/branch/HGBR6_DS {0} .joblib' . format ( dataset_number )) reg2 = load ( r 'sklearn_models6c/branch/RFR6_DS {0} .joblib' . format ( dataset_number )) reg3 = load ( r 'sklearn_models6c/branch/MLPR6_DS {0} .joblib' . format ( dataset_number )) # reg4 = load(r'sklearn_models6/SGDR1_DS1.joblib') ereg = VotingRegressor ([( 'HGBR6_DS {0} ' . format ( dataset_number ), reg1 ), ( 'RFR6_DS {0} ' . format ( dataset_number ), reg2 ), ( 'MLPR6_DS {0} ' . format ( dataset_number ), reg3 ) ], weights = [ 3. / 6. , 2. / 6. , 1. / 6. ]) ereg . fit ( self . X_train , self . y_train ) dump ( ereg , model_path ) self . evaluate_model ( model = ereg , model_path = model_path ) try : self . evaluate_model_class ( model = ereg , model_path = model_path ) except : print ( 'Prediction is out of the range.' ) regress ( self , model_path = None , model_number = None ) :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: Source code in adafest\\code\\testability\\ml_models_coverageability.py def regress ( self , model_path : str = None , model_number : int = None ): \"\"\" :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: \"\"\" regressor = None parameters = None if model_number == 1 : regressor = tree . DecisionTreeRegressor ( random_state = 42 , ) # Set the parameters to be used for tuning by cross-validation parameters = { # 'criterion': ['mse', 'friedman_mse', 'mae'], 'max_depth' : range ( 3 , 50 , 5 ), 'min_samples_split' : range ( 2 , 30 , 2 ) } elif model_number == 2 : regressor = RandomForestRegressor ( random_state = 42 , ) parameters = { 'n_estimators' : range ( 100 , 200 , 100 ), # 'criterion': ['mse', 'mae'], 'max_depth' : range ( 10 , 50 , 10 ), # 'min_samples_split': range(2, 30, 2), # 'max_features': ['auto', 'sqrt', 'log2'] } elif model_number == 3 : regressor = GradientBoostingRegressor ( n_estimators = 400 , learning_rate = 0.05 , random_state = 42 , ) parameters = { # 'loss': ['ls', 'lad', ], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_split' : range ( 2 , 30 , 3 ) } elif model_number == 4 : regressor = HistGradientBoostingRegressor ( max_iter = 400 , learning_rate = 0.05 , random_state = 42 , ) parameters = { # 'loss': ['least_squares', 'least_absolute_deviation'], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_leaf' : range ( 5 , 50 , 10 ) } elif model_number == 5 : regressor = linear_model . SGDRegressor ( early_stopping = True , n_iter_no_change = 5 , random_state = 42 , ) parameters = { 'loss' : [ 'squared_loss' , 'huber' , 'epsilon_insensitive' ], 'penalty' : [ 'l2' , 'l1' , 'elasticnet' ], 'max_iter' : range ( 50 , 1000 , 50 ), 'learning_rate' : [ 'invscaling' , 'optimal' , 'constant' , 'adaptive' ], 'eta0' : [ 0.1 , 0.01 ], 'average' : [ 32 , ] } elif model_number == 6 : regressor = MLPRegressor ( random_state = 42 , ) parameters = { 'hidden_layer_sizes' : [( 256 , 100 ), ( 512 , 256 , 100 ), ], 'activation' : [ 'tanh' , ], 'solver' : [ 'adam' , ], 'max_iter' : range ( 50 , 200 , 50 ) } if regressor is None : return if parameters is None : return # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.25 , random_state = 42 ) # Find the best model using gird-search with cross-validation clf = GridSearchCV ( regressor , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 4 , refit = 'neg_root_mean_squared_error' ) print ( 'fitting model number' , model_number ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # self.evaluate_model_class(model=clf, model_path=model_path) # self.inference_model(model=clf, model_path=model_path) print ( '=' * 75 ) create_coverageability_dataset_with_only_important_metrics () Create DS#6 (DS06610) For use in Mr Esmaeili project Select only top 15 important Coverageability features :return: Source code in adafest\\code\\testability\\ml_models_coverageability.py def create_coverageability_dataset_with_only_important_metrics (): \"\"\" Create DS#6 (DS06610) For use in Mr Esmaeili project Select only top 15 important Coverageability features :return: \"\"\" df_path = r 'dataset06/DS06013.csv' df_important_metrics_path = r 'dataset06/DS06610.csv' df = pd . read_csv ( df_path , delimiter = ',' , index_col = False ) df_imp = pd . DataFrame () df_imp [ 'Class' ] = df [ 'Class' ] # 0 df_imp [ 'CSORD_SumCyclomaticStrict' ] = df [ 'CSORD_SumCyclomaticStrict' ] # 1 df_imp [ 'CSLEX_NumberOfConditionalJumpStatements' ] = df [ 'CSLEX_NumberOfConditionalJumpStatements' ] # 2 df_imp [ 'CSORD_LogCyclomaticStrict' ] = df [ 'CSORD_LogCyclomaticStrict' ] # 3 df_imp [ 'CSORD_CSNOMNAMM' ] = df [ 'CSORD_CSNOMNAMM' ] # 4 df_imp [ 'CSORD_NIM' ] = df [ 'CSORD_NIM' ] # 5 df_imp [ 'CSORD_LogStmtDecl' ] = df [ 'CSORD_LogStmtDecl' ] # 6 df_imp [ 'CSORD_CountDeclMethodPrivate' ] = df [ 'CSORD_CountDeclMethodPrivate' ] # 7 df_imp [ 'CSORD_CountDeclClassMethod' ] = df [ 'CSORD_CountDeclClassMethod' ] # 8 df_imp [ 'CSORD_NumberOfClassConstructors' ] = df [ 'CSORD_NumberOfClassConstructors' ] # 9 df_imp [ 'CSORD_MinLineCode' ] = df [ 'CSORD_MinLineCode' ] # 10 df_imp [ 'CSORD_SumCyclomatic' ] = df [ 'CSORD_SumCyclomatic' ] # 11 df_imp [ 'CSLEX_NumberOfReturnAndPrintStatements' ] = df [ 'CSLEX_NumberOfReturnAndPrintStatements' ] # 12 df_imp [ 'CSORD_MaxInheritanceTree' ] = df [ 'CSORD_MaxInheritanceTree' ] # 13 df_imp [ 'CSLEX_NumberOfIdentifies' ] = df [ 'CSLEX_NumberOfIdentifies' ] # 14 df_imp [ 'CSORD_CountDeclMethodPublic' ] = df [ 'CSORD_CountDeclMethodPublic' ] # 15 # Runtime metrics df_imp [ 'Label_Combine1' ] = df [ 'Label_Combine1' ] df_imp [ 'Label_LineCoverage' ] = df [ 'Label_LineCoverage' ] df_imp [ 'Label_BranchCoverage' ] = df [ 'Label_BranchCoverage' ] df_imp [ 'Coverageability1' ] = df [ 'Coverageability1' ] df_imp [ 'Tests' ] = df [ 'Tests' ] df_imp . to_csv ( df_important_metrics_path , index = False ) train_on_ds6 () To be used for predict expected value of statement and branch coverage. index -1: Coveragability1 (i.e., Testability) index -2: E[C] = 0.5 branch + 0.5 line (Arithmetic mean) ==> models names: XXX1_DSX index -3: Test suite size index -4: BranchCoverage ==> model names: XXX2_DSX index -5: LineCoverage ==> model names: XXX3_DSX index new_col1: Coverageability (Arithmetic mean) ==> model names: XXX4_DSX index new_col2: Coverageability2 (Geometric mean) ==> model names: XXX5_DSX index new_col3: Coverageability3 (Harmonic mean) ==> model names: XXX6_DSX Source code in adafest\\code\\testability\\ml_models_coverageability.py def train_on_ds6 (): \"\"\" To be used for predict expected value of statement and branch coverage. index -1: Coveragability1 (i.e., Testability) index -2: E[C] = 0.5*branch + 0.5*line (Arithmetic mean) ==> models names: XXX1_DSX index -3: Test suite size index -4: BranchCoverage ==> model names: XXX2_DSX index -5: LineCoverage ==> model names: XXX3_DSX index new_col1: Coverageability (Arithmetic mean) ==> model names: XXX4_DSX index new_col2: Coverageability2 (Geometric mean) ==> model names: XXX5_DSX index new_col3: Coverageability3 (Harmonic mean) ==> model names: XXX6_DSX Returns: \"\"\" # DS1 # reg = Regression(df_path=r'dataset06/DS06013.csv') # reg.regress(model_path=r'sklearn_models6c/DTR1_DS1.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability3/RFR6_DS1.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/GBR1_DS1.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability3/HGBR6_DS1.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability3/SGDR6_DS1.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/statement/MLPR3_DS1.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/statement/VR3_DS1.joblib', dataset_number=1) # reg.evaluate_model(model_path=r'sklearn_models6/HGBR1_DS1.joblib',) # reg.inference_model2(model_path=r'sklearn_models6/VR1_DS1.joblib', # predict_data_path=r'dataset06/refactored01010.csv') # reg.inference_model2(model_path=r'sklearn_models6/VR1_DS1.joblib', # predict_data_path=r'D:/IdeaProjects/10_water-simulator/site_1/metrics1_1.csv') # quit() # DS 1/2 # reg.regress(model_path=r'sklearn_models6c/DTR1_DS2.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability3/RFR6_DS2.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/GBR1_DS2.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability3/HGBR6_DS2.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability3/SGDR6_DS2.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/coveragability3/MLPR6_DS2.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/coveragability3/VR6_DS2.joblib', dataset_number=2) # quit() # DS 3 # reg = Regression(df_path=r'dataset06/DS06310.csv') # reg.regress(model_path=r'sklearn_models6c/DTR1_DS3.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability3/RFR6_DS3.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/GBR1_DS3.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability3/HGBR6_DS3.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability3/SGDR6_DS3.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/statement/MLPR3_DS3.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/statement/VR3_DS3.joblib', dataset_number=3) # DS 4 # reg = Regression(df_path=r'dataset06/DS06410.csv') # reg.regress(model_path=r'sklearn_models6c/DTR1_DS4.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability3/RFR6_DS4.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/GBR1_DS4.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability3/HGBR6_DS4.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability3/SGDR6_DS4.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/statement/MLPR3_DS4.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/statement/VR3_DS4.joblib', dataset_number=4) # DS5 reg = Regression ( df_path = r 'dataset06/DS06510.csv' ) # reg.regress(model_path=r'sklearn_models6c/branch/DTR6_DS5.joblib', model_number=1) reg . regress ( model_path = r 'sklearn_models6c/branch/RFR6_DS5.joblib' , model_number = 2 ) # reg.regress(model_path=r'sklearn_models6c/branch/GBR6_DS5.joblib', model_number=3) reg . regress ( model_path = r 'sklearn_models6c/branch/HGBR6_DS5.joblib' , model_number = 4 ) reg . regress ( model_path = r 'sklearn_models6c/branch/SGDR6_DS5.joblib' , model_number = 5 ) reg . regress ( model_path = r 'sklearn_models6c/branch/MLPR6_DS5.joblib' , model_number = 6 ) reg . vote ( model_path = r 'sklearn_models6c/branch/VR6_DS5.joblib' , dataset_number = 5 ) # quit() # Added for Mr. Esmaeily work # DS6 (important metrics) df_important_metrics_path = r 'dataset06/DS06610.csv' reg = Regression ( df_path = df_important_metrics_path ) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/DTR6_DS6.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/RFR6_DS6.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/GBR6_DS6.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/HGBR6_DS6.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/SGDR6_DS6.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/MLPR6_DS6.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/VR6_DS6.joblib', dataset_number=6) model_path = r 'sklearn_models6c/coveragability/VR4_DS3.joblib' scoring = [ 'r2' , 'neg_mean_absolute_error' , 'neg_mean_squared_error' , 'neg_median_absolute_error' ] n_repeat = [ 10 , 20 , 30 , 40 , 50 ] for score in scoring : for r in n_repeat : compute_permutation_importance ( model_path = model_path , scoring = score , n_repeats = r , )","title":"Coverageability prediction"},{"location":"modules/coverageability_prediction/#coverageability-prediction","text":"The replication package for the paper 'Learning to predict test effectiveness' published in International Journal of Intelligent Systems.","title":"Coverageability prediction"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability--goal","text":"This script implements machine learning models for predicting the expected value of statement and branch coverage presented in International Journal of Intelligent Systems.","title":"Goal"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability--machine-learning-models","text":"Model 1: DecisionTreeRegressor Model 2: RandomForestRegressor Model 3: GradientBoostingRegressor Model 4: HistGradientBoostingRegressor Model 5: SGDRegressor Model 6: MLPRegressor","title":"Machine learning models"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability--learning-datasets","text":"Dataset Applied preprocessing Number of metrics DS1: (default) Simple classes elimination, data classes elimination, outliers elimination, and metric standardization 262 DS2: DS1 + Feature selection 20 DS3: DS1 + Context vector elimination 194 DS4: DS1 + Context vector elimination and lexical metrics elimination 177 DS5: DS1 + Systematically generated metrics elimination 71 DS6: Top 15 important source code metrics affecting Coverageability","title":"Learning datasets"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability--model-dependent-variable","text":"E[C] = (1/2 Statement coverage + 1/2 Branch coverage) * b/|n|","title":"Model dependent variable"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability--results","text":"The results will be saved in sklearn_models6c","title":"Results"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability--inferences","text":"Use the method inference_model2 of the class Regression to predict testability of new Java classes","title":"Inferences"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability.MultioutputClassification","text":"https://scikit-learn.org/stable/modules/multiclass.html#multioutput-classification Multioutput-multiclass classification (also known as multitask classification) Source code in adafest\\code\\testability\\ml_models_coverageability.py class MultioutputClassification : \"\"\" https://scikit-learn.org/stable/modules/multiclass.html#multioutput-classification Multioutput-multiclass classification (also known as multitask classification) \"\"\" pass","title":"MultioutputClassification"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability.Regression","text":"Source code in adafest\\code\\testability\\ml_models_coverageability.py class Regression ( object ): def __init__ ( self , df_path = r 'dataset06/DS06013.csv' , avg_type = None ): self . df = pd . read_csv ( df_path , delimiter = ',' , index_col = False ) self . df [ 'Label_Combine1' ] = self . df [ 'Label_Combine1' ] * 0.01 self . df [ 'Label_LineCoverage' ] = self . df [ 'Label_LineCoverage' ] * 0.01 self . df [ 'Label_BranchCoverage' ] = self . df [ 'Label_BranchCoverage' ] * 0.01 self . df [ 'Coverageability1' ] = self . df [ 'Coverageability1' ] * 0.01 label_coverageability = self . df [ 'Label_Combine1' ] / self . df [ 'Tests' ] # (Arithmetic mean) if avg_type is not None : label_coverageability2 = list () # (Geometric mean) label_coverageability3 = list () # (Harmonic mean) for row in self . df . iterrows (): print ( row [ 1 ][ - 3 ]) label_coverageability2 . append ( ( math . sqrt ( row [ 1 ][ - 4 ] * row [ 1 ][ - 5 ])) / row [ 1 ][ - 3 ] ) # (Geometric mean) label_coverageability3 . append ( (( 2 * row [ 1 ][ - 4 ] * row [ 1 ][ - 5 ]) / ( row [ 1 ][ - 4 ] + row [ 1 ][ - 5 ])) / row [ 1 ][ - 3 ] ) # (Harmonic mean) label_coverageability2 = pd . DataFrame ( label_coverageability2 ) label_coverageability3 = pd . DataFrame ( label_coverageability3 ) # print('Before applying filter:', self.df.shape) # self.df = self.df.loc[(self.df.Label_BranchCoverage <= 0.50)] # self.df = self.df.loc[(self.df.Label_LineCoverage <= 0.50)] # print('After applying filter:', self.df.shape) # index -1: Coveragability1 (i.e., Testability) # index -2: E[C] = 1/2 branch * line ==> models names: XXX1_DSX # index -3: Test suite size # index -4: BranchCoverage ==> model names: XXX2_DSX # index -5: LineCoverage ==> model names: XXX3_DSX self . X_train1 , self . X_test1 , self . y_train , self . y_test = train_test_split ( self . df . iloc [:, 1 : - 5 ], # self.df.iloc[:, -2], # label_coverageability, self . df [ 'Label_BranchCoverage' ], test_size = 0.25 , random_state = 42 , # stratify=self.df.iloc[:, -1] ) \"\"\" # --------------------------------------- # -- Feature selection (For DS2) selector = feature_selection.SelectKBest(feature_selection.f_regression, k=15) # clf = linear_model.LassoCV(eps=1e-3, n_alphas=100, normalize=True, max_iter=5000, tol=1e-4) # clf.fit(self.X_train1, self.y_train) # importance = np.abs(clf.coef_) # print('importance', importance) # clf = RandomForestRegressor() # selector = feature_selection.SelectFromModel(clf, prefit=False, norm_order=2, max_features=20, threshold=None) selector.fit(self.X_train1, self.y_train) # Get columns to keep and create new dataframe with only selected features cols = selector.get_support(indices=True) self.X_train1 = self.X_train1.iloc[:, cols] self.X_test1 = self.X_test1.iloc[:, cols] print('Selected columns by feature selection:', self.X_train1.columns) # quit() # -- End of feature selection \"\"\" # --------------------------------------- # Standardization self . scaler = preprocessing . RobustScaler ( with_centering = True , with_scaling = True ) # self.scaler = preprocessing.StandardScaler() self . scaler . fit ( self . X_train1 ) self . X_train = self . scaler . transform ( self . X_train1 ) self . X_test = self . scaler . transform ( self . X_test1 ) dump ( self . scaler , 'DS06510.joblib' ) # quit() def inference_model ( self , model = None , model_path = None ): if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test [ 3 : 4 , ]) print ( 'X_test {0} ' . format ( self . X_test [ 3 : 4 , ])) print ( '------' ) print ( 'y_test or y_true {0} ' . format ( y_true [ 3 : 4 , ])) print ( '------' ) print ( 'y_pred by model {0} ' . format ( y_pred )) y_true , y_pred = self . y_test , model . predict ( self . X_test ) df_new = pd . DataFrame ( columns = self . df . columns ) for i , row in self . y_test . iteritems (): print ( '' , i , row ) df_new = df_new . append ( self . df . loc [ i ], ignore_index = True ) df_new [ 'y_true' ] = self . y_test . values df_new [ 'y_pred' ] = list ( y_pred ) df_new . to_csv ( model_path [: - 7 ] + '_inference_result.csv' , index = True , index_label = 'Row' ) def inference_model2 ( self , model = None , model_path = None , predict_data_path = None ): if model is None : model = joblib . load ( model_path ) df_predict_data = pd . read_csv ( predict_data_path , delimiter = ',' , index_col = False ) X_test1 = df_predict_data . iloc [:, 1 :] X_test = self . scaler . transform ( X_test1 ) y_pred = model . predict ( X_test ) df_new = pd . DataFrame ( df_predict_data . iloc [:, 0 ], columns = [ 'Class' ]) df_new [ 'PredictedTestability' ] = list ( y_pred ) print ( df_new ) # df_new.to_csv(r'dataset06/refactored01010_predicted_testability.csv', index=True, index_label='Row') def evaluate_model ( self , model = None , model_path = None ): # X = self.data_frame.iloc[:, 1:-4] # y = self.data_frame.iloc[:, -4] # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0) if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test ) # y_score = model.predict_proba(X_test) # Print all classifier model metrics print ( 'Evaluating regressor ...' ) print ( 'Regressor minimum prediction' , min ( y_pred ), 'Regressor maximum prediction' , max ( y_pred )) df = pd . DataFrame () df [ 'r2_score_uniform_average' ] = [ r2_score ( y_true , y_pred , multioutput = 'uniform_average' )] df [ 'r2_score_variance_weighted' ] = [ r2_score ( y_true , y_pred , multioutput = 'variance_weighted' )] df [ 'explained_variance_score_uniform_average' ] = [ explained_variance_score ( y_true , y_pred , multioutput = 'uniform_average' )] df [ 'explained_variance_score_variance_weighted' ] = [ explained_variance_score ( y_true , y_pred , multioutput = 'variance_weighted' )] df [ 'mean_absolute_error' ] = [ mean_absolute_error ( y_true , y_pred )] df [ 'mean_squared_error_MSE' ] = [ mean_squared_error ( y_true , y_pred )] df [ 'mean_squared_error_RMSE' ] = [ mean_squared_error ( y_true , y_pred , squared = False )] df [ 'median_absolute_error' ] = [ median_absolute_error ( y_true , y_pred )] if min ( y_pred ) >= 0 : df [ 'mean_squared_log_error' ] = [ mean_squared_log_error ( y_true , y_pred )] # To handl ValueError: Mean Tweedie deviance error with power=2 # can only be used on strictly positive y and y_pred. if min ( y_pred > 0 ) and min ( y_true ) > 0 : df [ 'mean_poisson_deviance' ] = [ mean_poisson_deviance ( y_true , y_pred , )] df [ 'mean_gamma_deviance' ] = [ mean_gamma_deviance ( y_true , y_pred , )] df [ 'max_error' ] = [ max_error ( y_true , y_pred )] df . to_csv ( model_path [: - 7 ] + '_evaluation_metrics_R1.csv' , index = True , index_label = 'Row' ) def evaluate_model_class ( self , model = None , model_path = None ): if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test ) df_new = pd . DataFrame ( y_true ) df_new [ 'y_pred' ] = y_pred testability_labels = [ 'VeryLow' , 'Low' , 'Moderate' , 'High' , 'VeryHigh' ] testability_labels = [ 'Low' , 'Moderate' , 'High' ] bins = [ - 1.250 , 0.250 , 0.750 , 1.250 ] # bins = 5 df_new [ 'y_ture_nominal' ] = pd . cut ( df_new . loc [:, [ 'Coverageability1' ]] . T . squeeze (), bins = bins , labels = testability_labels , right = True ) df_new [ 'y_pred_nominal' ] = pd . cut ( df_new . loc [:, [ 'y_pred' ]] . T . squeeze (), bins = bins , labels = testability_labels , right = True ) print ( df_new ) # df_new.to_csv('XXXXX.csv') y_true = df_new [ 'y_ture_nominal' ] y_pred = df_new [ 'y_pred_nominal' ] y_score = y_pred # Print all classifier model metrics print ( 'Evaluating classifier ...' ) df = pd . DataFrame () print ( y_pred ) try : df [ 'accuracy_score' ] = [ accuracy_score ( y_true , y_pred )] df [ 'balanced_accuracy_score' ] = [ balanced_accuracy_score ( y_true , y_pred )] df [ 'precision_score_macro' ] = [ precision_score ( y_true , y_pred , average = 'macro' )] df [ 'precision_score_micro' ] = [ precision_score ( y_true , y_pred , average = 'micro' )] df [ 'recall_score_macro' ] = [ recall_score ( y_true , y_pred , average = 'macro' )] df [ 'recall_score_micro' ] = [ recall_score ( y_true , y_pred , average = 'micro' )] df [ 'f1_score_macro' ] = [ f1_score ( y_true , y_pred , average = 'macro' )] df [ 'f1_score_micro' ] = [ f1_score ( y_true , y_pred , average = 'micro' )] df [ 'fbeta_score_macro' ] = [ fbeta_score ( y_true , y_pred , beta = 0.5 , average = 'macro' )] df [ 'fbeta_score_micro' ] = [ fbeta_score ( y_true , y_pred , beta = 0.5 , average = 'micro' )] # df['log_loss'] = [log_loss(y_true, y_score)] # df['roc_auc_score_ovr_macro'] = [roc_auc_score(y_true, y_score, multi_class='ovr', average='macro')] # df['roc_auc_score_ovr_micro'] = [roc_auc_score(y_true, y_score, multi_class='ovr', average='weighted')] # df['roc_auc_score_ovo_macro'] = [roc_auc_score(y_true, y_score, multi_class='ovo', average='macro')] # df['roc_auc_score_ovo_micro'] = [roc_auc_score(y_true, y_score, multi_class='ovo', average='weighted')] # print('roc_curve_:', roc_curve(y_true, y_score)) # multiclass format is not supported df . to_csv ( model_path [: - 7 ] + '_evaluation_metrics_C.csv' , index = True , index_label = 'Row' ) except : raise ValueError ( 'The prediction is out of range' ) def regress_with_decision_tree ( self , model_path ): # X = self.data_frame.iloc[:, 1:-4] # y = self.data_frame.iloc[:, -4] # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0) clf = tree . DecisionTreeRegressor () # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.25 , random_state = 42 ) # Set the parameters to be used for tuning by cross-validation parameters = { 'max_depth' : range ( 1 , 100 , 10 ), 'criterion' : [ 'mse' , 'friedman_mse' , 'mae' ], 'min_samples_split' : range ( 2 , 20 , 1 ) } # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # Find the best model using gird-search with cross-validation clf = GridSearchCV ( clf , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 4 , refit = 'neg_root_mean_squared_error' ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # Plots # tree.plot_tree(clf) # plt.show() def regress ( self , model_path : str = None , model_number : int = None ): \"\"\" :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: \"\"\" regressor = None parameters = None if model_number == 1 : regressor = tree . DecisionTreeRegressor ( random_state = 42 , ) # Set the parameters to be used for tuning by cross-validation parameters = { # 'criterion': ['mse', 'friedman_mse', 'mae'], 'max_depth' : range ( 3 , 50 , 5 ), 'min_samples_split' : range ( 2 , 30 , 2 ) } elif model_number == 2 : regressor = RandomForestRegressor ( random_state = 42 , ) parameters = { 'n_estimators' : range ( 100 , 200 , 100 ), # 'criterion': ['mse', 'mae'], 'max_depth' : range ( 10 , 50 , 10 ), # 'min_samples_split': range(2, 30, 2), # 'max_features': ['auto', 'sqrt', 'log2'] } elif model_number == 3 : regressor = GradientBoostingRegressor ( n_estimators = 400 , learning_rate = 0.05 , random_state = 42 , ) parameters = { # 'loss': ['ls', 'lad', ], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_split' : range ( 2 , 30 , 3 ) } elif model_number == 4 : regressor = HistGradientBoostingRegressor ( max_iter = 400 , learning_rate = 0.05 , random_state = 42 , ) parameters = { # 'loss': ['least_squares', 'least_absolute_deviation'], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_leaf' : range ( 5 , 50 , 10 ) } elif model_number == 5 : regressor = linear_model . SGDRegressor ( early_stopping = True , n_iter_no_change = 5 , random_state = 42 , ) parameters = { 'loss' : [ 'squared_loss' , 'huber' , 'epsilon_insensitive' ], 'penalty' : [ 'l2' , 'l1' , 'elasticnet' ], 'max_iter' : range ( 50 , 1000 , 50 ), 'learning_rate' : [ 'invscaling' , 'optimal' , 'constant' , 'adaptive' ], 'eta0' : [ 0.1 , 0.01 ], 'average' : [ 32 , ] } elif model_number == 6 : regressor = MLPRegressor ( random_state = 42 , ) parameters = { 'hidden_layer_sizes' : [( 256 , 100 ), ( 512 , 256 , 100 ), ], 'activation' : [ 'tanh' , ], 'solver' : [ 'adam' , ], 'max_iter' : range ( 50 , 200 , 50 ) } if regressor is None : return if parameters is None : return # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.25 , random_state = 42 ) # Find the best model using gird-search with cross-validation clf = GridSearchCV ( regressor , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 4 , refit = 'neg_root_mean_squared_error' ) print ( 'fitting model number' , model_number ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # self.evaluate_model_class(model=clf, model_path=model_path) # self.inference_model(model=clf, model_path=model_path) print ( '=' * 75 ) def vote ( self , model_path = None , dataset_number = 1 ): # Trained regressors reg1 = load ( r 'sklearn_models6c/branch/HGBR6_DS {0} .joblib' . format ( dataset_number )) reg2 = load ( r 'sklearn_models6c/branch/RFR6_DS {0} .joblib' . format ( dataset_number )) reg3 = load ( r 'sklearn_models6c/branch/MLPR6_DS {0} .joblib' . format ( dataset_number )) # reg4 = load(r'sklearn_models6/SGDR1_DS1.joblib') ereg = VotingRegressor ([( 'HGBR6_DS {0} ' . format ( dataset_number ), reg1 ), ( 'RFR6_DS {0} ' . format ( dataset_number ), reg2 ), ( 'MLPR6_DS {0} ' . format ( dataset_number ), reg3 ) ], weights = [ 3. / 6. , 2. / 6. , 1. / 6. ]) ereg . fit ( self . X_train , self . y_train ) dump ( ereg , model_path ) self . evaluate_model ( model = ereg , model_path = model_path ) try : self . evaluate_model_class ( model = ereg , model_path = model_path ) except : print ( 'Prediction is out of the range.' )","title":"Regression"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability.Regression.regress","text":":param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: Source code in adafest\\code\\testability\\ml_models_coverageability.py def regress ( self , model_path : str = None , model_number : int = None ): \"\"\" :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: \"\"\" regressor = None parameters = None if model_number == 1 : regressor = tree . DecisionTreeRegressor ( random_state = 42 , ) # Set the parameters to be used for tuning by cross-validation parameters = { # 'criterion': ['mse', 'friedman_mse', 'mae'], 'max_depth' : range ( 3 , 50 , 5 ), 'min_samples_split' : range ( 2 , 30 , 2 ) } elif model_number == 2 : regressor = RandomForestRegressor ( random_state = 42 , ) parameters = { 'n_estimators' : range ( 100 , 200 , 100 ), # 'criterion': ['mse', 'mae'], 'max_depth' : range ( 10 , 50 , 10 ), # 'min_samples_split': range(2, 30, 2), # 'max_features': ['auto', 'sqrt', 'log2'] } elif model_number == 3 : regressor = GradientBoostingRegressor ( n_estimators = 400 , learning_rate = 0.05 , random_state = 42 , ) parameters = { # 'loss': ['ls', 'lad', ], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_split' : range ( 2 , 30 , 3 ) } elif model_number == 4 : regressor = HistGradientBoostingRegressor ( max_iter = 400 , learning_rate = 0.05 , random_state = 42 , ) parameters = { # 'loss': ['least_squares', 'least_absolute_deviation'], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_leaf' : range ( 5 , 50 , 10 ) } elif model_number == 5 : regressor = linear_model . SGDRegressor ( early_stopping = True , n_iter_no_change = 5 , random_state = 42 , ) parameters = { 'loss' : [ 'squared_loss' , 'huber' , 'epsilon_insensitive' ], 'penalty' : [ 'l2' , 'l1' , 'elasticnet' ], 'max_iter' : range ( 50 , 1000 , 50 ), 'learning_rate' : [ 'invscaling' , 'optimal' , 'constant' , 'adaptive' ], 'eta0' : [ 0.1 , 0.01 ], 'average' : [ 32 , ] } elif model_number == 6 : regressor = MLPRegressor ( random_state = 42 , ) parameters = { 'hidden_layer_sizes' : [( 256 , 100 ), ( 512 , 256 , 100 ), ], 'activation' : [ 'tanh' , ], 'solver' : [ 'adam' , ], 'max_iter' : range ( 50 , 200 , 50 ) } if regressor is None : return if parameters is None : return # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.25 , random_state = 42 ) # Find the best model using gird-search with cross-validation clf = GridSearchCV ( regressor , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 4 , refit = 'neg_root_mean_squared_error' ) print ( 'fitting model number' , model_number ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # self.evaluate_model_class(model=clf, model_path=model_path) # self.inference_model(model=clf, model_path=model_path) print ( '=' * 75 )","title":"regress()"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability.create_coverageability_dataset_with_only_important_metrics","text":"Create DS#6 (DS06610) For use in Mr Esmaeili project Select only top 15 important Coverageability features :return: Source code in adafest\\code\\testability\\ml_models_coverageability.py def create_coverageability_dataset_with_only_important_metrics (): \"\"\" Create DS#6 (DS06610) For use in Mr Esmaeili project Select only top 15 important Coverageability features :return: \"\"\" df_path = r 'dataset06/DS06013.csv' df_important_metrics_path = r 'dataset06/DS06610.csv' df = pd . read_csv ( df_path , delimiter = ',' , index_col = False ) df_imp = pd . DataFrame () df_imp [ 'Class' ] = df [ 'Class' ] # 0 df_imp [ 'CSORD_SumCyclomaticStrict' ] = df [ 'CSORD_SumCyclomaticStrict' ] # 1 df_imp [ 'CSLEX_NumberOfConditionalJumpStatements' ] = df [ 'CSLEX_NumberOfConditionalJumpStatements' ] # 2 df_imp [ 'CSORD_LogCyclomaticStrict' ] = df [ 'CSORD_LogCyclomaticStrict' ] # 3 df_imp [ 'CSORD_CSNOMNAMM' ] = df [ 'CSORD_CSNOMNAMM' ] # 4 df_imp [ 'CSORD_NIM' ] = df [ 'CSORD_NIM' ] # 5 df_imp [ 'CSORD_LogStmtDecl' ] = df [ 'CSORD_LogStmtDecl' ] # 6 df_imp [ 'CSORD_CountDeclMethodPrivate' ] = df [ 'CSORD_CountDeclMethodPrivate' ] # 7 df_imp [ 'CSORD_CountDeclClassMethod' ] = df [ 'CSORD_CountDeclClassMethod' ] # 8 df_imp [ 'CSORD_NumberOfClassConstructors' ] = df [ 'CSORD_NumberOfClassConstructors' ] # 9 df_imp [ 'CSORD_MinLineCode' ] = df [ 'CSORD_MinLineCode' ] # 10 df_imp [ 'CSORD_SumCyclomatic' ] = df [ 'CSORD_SumCyclomatic' ] # 11 df_imp [ 'CSLEX_NumberOfReturnAndPrintStatements' ] = df [ 'CSLEX_NumberOfReturnAndPrintStatements' ] # 12 df_imp [ 'CSORD_MaxInheritanceTree' ] = df [ 'CSORD_MaxInheritanceTree' ] # 13 df_imp [ 'CSLEX_NumberOfIdentifies' ] = df [ 'CSLEX_NumberOfIdentifies' ] # 14 df_imp [ 'CSORD_CountDeclMethodPublic' ] = df [ 'CSORD_CountDeclMethodPublic' ] # 15 # Runtime metrics df_imp [ 'Label_Combine1' ] = df [ 'Label_Combine1' ] df_imp [ 'Label_LineCoverage' ] = df [ 'Label_LineCoverage' ] df_imp [ 'Label_BranchCoverage' ] = df [ 'Label_BranchCoverage' ] df_imp [ 'Coverageability1' ] = df [ 'Coverageability1' ] df_imp [ 'Tests' ] = df [ 'Tests' ] df_imp . to_csv ( df_important_metrics_path , index = False )","title":"create_coverageability_dataset_with_only_important_metrics()"},{"location":"modules/coverageability_prediction/#adafest.code.testability.ml_models_coverageability.train_on_ds6","text":"To be used for predict expected value of statement and branch coverage. index -1: Coveragability1 (i.e., Testability) index -2: E[C] = 0.5 branch + 0.5 line (Arithmetic mean) ==> models names: XXX1_DSX index -3: Test suite size index -4: BranchCoverage ==> model names: XXX2_DSX index -5: LineCoverage ==> model names: XXX3_DSX index new_col1: Coverageability (Arithmetic mean) ==> model names: XXX4_DSX index new_col2: Coverageability2 (Geometric mean) ==> model names: XXX5_DSX index new_col3: Coverageability3 (Harmonic mean) ==> model names: XXX6_DSX Source code in adafest\\code\\testability\\ml_models_coverageability.py def train_on_ds6 (): \"\"\" To be used for predict expected value of statement and branch coverage. index -1: Coveragability1 (i.e., Testability) index -2: E[C] = 0.5*branch + 0.5*line (Arithmetic mean) ==> models names: XXX1_DSX index -3: Test suite size index -4: BranchCoverage ==> model names: XXX2_DSX index -5: LineCoverage ==> model names: XXX3_DSX index new_col1: Coverageability (Arithmetic mean) ==> model names: XXX4_DSX index new_col2: Coverageability2 (Geometric mean) ==> model names: XXX5_DSX index new_col3: Coverageability3 (Harmonic mean) ==> model names: XXX6_DSX Returns: \"\"\" # DS1 # reg = Regression(df_path=r'dataset06/DS06013.csv') # reg.regress(model_path=r'sklearn_models6c/DTR1_DS1.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability3/RFR6_DS1.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/GBR1_DS1.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability3/HGBR6_DS1.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability3/SGDR6_DS1.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/statement/MLPR3_DS1.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/statement/VR3_DS1.joblib', dataset_number=1) # reg.evaluate_model(model_path=r'sklearn_models6/HGBR1_DS1.joblib',) # reg.inference_model2(model_path=r'sklearn_models6/VR1_DS1.joblib', # predict_data_path=r'dataset06/refactored01010.csv') # reg.inference_model2(model_path=r'sklearn_models6/VR1_DS1.joblib', # predict_data_path=r'D:/IdeaProjects/10_water-simulator/site_1/metrics1_1.csv') # quit() # DS 1/2 # reg.regress(model_path=r'sklearn_models6c/DTR1_DS2.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability3/RFR6_DS2.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/GBR1_DS2.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability3/HGBR6_DS2.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability3/SGDR6_DS2.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/coveragability3/MLPR6_DS2.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/coveragability3/VR6_DS2.joblib', dataset_number=2) # quit() # DS 3 # reg = Regression(df_path=r'dataset06/DS06310.csv') # reg.regress(model_path=r'sklearn_models6c/DTR1_DS3.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability3/RFR6_DS3.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/GBR1_DS3.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability3/HGBR6_DS3.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability3/SGDR6_DS3.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/statement/MLPR3_DS3.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/statement/VR3_DS3.joblib', dataset_number=3) # DS 4 # reg = Regression(df_path=r'dataset06/DS06410.csv') # reg.regress(model_path=r'sklearn_models6c/DTR1_DS4.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability3/RFR6_DS4.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/GBR1_DS4.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability3/HGBR6_DS4.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability3/SGDR6_DS4.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/statement/MLPR3_DS4.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/statement/VR3_DS4.joblib', dataset_number=4) # DS5 reg = Regression ( df_path = r 'dataset06/DS06510.csv' ) # reg.regress(model_path=r'sklearn_models6c/branch/DTR6_DS5.joblib', model_number=1) reg . regress ( model_path = r 'sklearn_models6c/branch/RFR6_DS5.joblib' , model_number = 2 ) # reg.regress(model_path=r'sklearn_models6c/branch/GBR6_DS5.joblib', model_number=3) reg . regress ( model_path = r 'sklearn_models6c/branch/HGBR6_DS5.joblib' , model_number = 4 ) reg . regress ( model_path = r 'sklearn_models6c/branch/SGDR6_DS5.joblib' , model_number = 5 ) reg . regress ( model_path = r 'sklearn_models6c/branch/MLPR6_DS5.joblib' , model_number = 6 ) reg . vote ( model_path = r 'sklearn_models6c/branch/VR6_DS5.joblib' , dataset_number = 5 ) # quit() # Added for Mr. Esmaeily work # DS6 (important metrics) df_important_metrics_path = r 'dataset06/DS06610.csv' reg = Regression ( df_path = df_important_metrics_path ) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/DTR6_DS6.joblib', model_number=1) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/RFR6_DS6.joblib', model_number=2) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/GBR6_DS6.joblib', model_number=3) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/HGBR6_DS6.joblib', model_number=4) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/SGDR6_DS6.joblib', model_number=5) # reg.regress(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/MLPR6_DS6.joblib', model_number=6) # reg.vote(model_path=r'sklearn_models6c/coveragability_arithmetic_mean/VR6_DS6.joblib', dataset_number=6) model_path = r 'sklearn_models6c/coveragability/VR4_DS3.joblib' scoring = [ 'r2' , 'neg_mean_absolute_error' , 'neg_mean_squared_error' , 'neg_median_absolute_error' ] n_repeat = [ 10 , 20 , 30 , 40 , 50 ] for score in scoring : for r in n_repeat : compute_permutation_importance ( model_path = model_path , scoring = score , n_repeats = r , )","title":"train_on_ds6()"},{"location":"modules/testability_prediction/","text":"Testability prediction The replication package for the paper 'An ensemble meta-estimator to predict source code testability' published in Applied Soft Computing Journal. Goal This script implements machine learning models for predicting testability presented in Applied Soft Computing Journal Machine learning models Model 1: DecisionTreeRegressor Model 2: RandomForestRegressor Model 3: GradientBoostingRegressor Model 4: HistGradientBoostingRegressor Model 5: SGDRegressor Model 6: MLPRegressor Learning datasets Dataset: Applied preprocessing: Number of metrics DS1: (default) Simple classes elimination, data classes elimination, outliers elimination, and metric standardization: 262 features DS2: DS1 + Feature selection: 20 features DS3: DS1 + Context vector elimination: 194 features DS4: DS1 + Context vector elimination and lexical metrics elimination 177 DS5: DS1 + Systematically generated metrics elimination 71 DS6: Top 15 important source code metrics affecting testability Model dependent variable Testability of class X: T(X) = E[C]/ (1 + omega) ^ (|n| - 1) where E[C] = 1/3 StatementCoverage + 1/3 BranchCoverage + 1/3*MutationCoverage Results The results will be saved in ../../results directory Inferences Use the method inference_model2 of the class Regression to predict testability of new Java classes. Regression Source code in adafest\\code\\testability\\ml_models_testability.py class Regression : def __init__ ( self , df_path : str = None , feature_selection_mode = False ): self . df = pd . read_csv ( df_path , delimiter = ',' , index_col = False ) self . X_train1 , self . X_test1 , self . y_train , self . y_test = train_test_split ( self . df . iloc [:, 1 : - 1 ], self . df . iloc [:, - 1 ], test_size = 0.25 , random_state = 117 , ) # -- Feature selection (For DS2) if feature_selection_mode : selector = feature_selection . SelectKBest ( feature_selection . f_regression , k = 20 ) # clf = linear_model.LassoCV(eps=1e-3, n_alphas=100, normalize=True, max_iter=5000, tol=1e-4) # clf.fit(self.X_train1, self.y_train) # importance = np.abs(clf.coef_) # print('importance', importance) # clf = RandomForestRegressor() # selector = feature_selection.SelectFromModel(clf, prefit=False, norm_order=2, max_features=20,) selector . fit ( self . X_train1 , self . y_train ) # Get columns to keep and create new dataframe with only selected features cols = selector . get_support ( indices = True ) self . X_train1 = self . X_train1 . iloc [:, cols ] self . X_test1 = self . X_test1 . iloc [:, cols ] print ( 'Selected columns by feature selection:' , self . X_train1 . columns ) # quit() # --- End of feature selection # Standardization # self.scaler = preprocessing.RobustScaler(with_centering=True, with_scaling=True, unit_variance=True) # self.scaler = preprocessing.StandardScaler() self . scaler = QuantileTransformer ( n_quantiles = 1000 , random_state = 11 ) self . scaler . fit ( self . X_train1 ) self . X_train = self . scaler . transform ( self . X_train1 ) self . X_test = self . scaler . transform ( self . X_test1 ) dump ( self . scaler , df_path [: - 4 ] + '_scaler.joblib' ) # quit() def regress ( self , model_path : str = None , model_number : int = None ): \"\"\" :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: \"\"\" regressor = None parameters = None if model_number == 1 : regressor = DecisionTreeRegressor ( random_state = 23 , ) # Set the parameters to be used for tuning by cross-validation parameters = { # 'criterion': ['mse', 'friedman_mse', 'mae'], 'max_depth' : range ( 3 , 50 , 5 ), 'min_samples_split' : range ( 2 , 30 , 2 ) } elif model_number == 2 : regressor = RandomForestRegressor ( random_state = 19 , ) parameters = { 'n_estimators' : range ( 100 , 200 , 100 ), # 'criterion': ['mse', 'mae'], 'max_depth' : range ( 10 , 50 , 10 ), # 'min_samples_split': range(2, 30, 2), # 'max_features': ['auto', 'sqrt', 'log2'] } elif model_number == 3 : regressor = GradientBoostingRegressor ( n_estimators = 400 , learning_rate = 0.05 , random_state = 17 , ) parameters = { # 'loss': ['ls', 'lad', ], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_split' : range ( 2 , 30 , 3 ) } elif model_number == 4 : regressor = HistGradientBoostingRegressor ( max_iter = 400 , learning_rate = 0.05 , random_state = 13 , ) parameters = { # 'loss': ['least_squares', 'least_absolute_deviation'], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_leaf' : range ( 5 , 50 , 10 ) } elif model_number == 5 : regressor = linear_model . SGDRegressor ( early_stopping = True , n_iter_no_change = 5 , random_state = 11 , ) parameters = { 'loss' : [ 'squared_loss' , 'huber' , 'epsilon_insensitive' ], 'penalty' : [ 'l2' , 'l1' , 'elasticnet' ], 'max_iter' : range ( 50 , 1000 , 50 ), 'learning_rate' : [ 'invscaling' , 'optimal' , 'constant' , 'adaptive' ], 'eta0' : [ 0.1 , 0.01 ], 'average' : [ 32 , ] } elif model_number == 6 : regressor = MLPRegressor ( random_state = 7 , ) parameters = { 'hidden_layer_sizes' : [( 256 , 100 ), ( 512 , 256 , 100 ), ], 'activation' : [ 'tanh' , ], 'solver' : [ 'adam' , ], 'max_iter' : range ( 50 , 200 , 50 ) } if regressor is None : return if parameters is None : return # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.20 , random_state = 101 ) # Find the best model using gird-search with cross-validation clf = GridSearchCV ( regressor , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 7 , refit = 'neg_root_mean_squared_error' ) print ( 'fitting model number' , model_number ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # self.evaluate_model_class(model=clf, model_path=model_path) # self.inference_model(model=clf, model_path=model_path) print ( '=' * 75 ) def vote ( self , model_path = None , dataset_number = 1 ): # Trained regressors reg1 = load ( r '../../results/HGBR1_DS {0} .joblib' . format ( dataset_number )) reg2 = load ( r '../../results/RFR1_DS {0} .joblib' . format ( dataset_number )) reg3 = load ( r '../../results/MLPR1_DS {0} .joblib' . format ( dataset_number )) # reg4 = load(r'results/SGDR1_DS1.joblib') ereg = VotingRegressor ( [( 'HGBR1_DS {0} ' . format ( dataset_number ), reg1 ), ( 'RFR1_DS {0} ' . format ( dataset_number ), reg2 ), ( 'MLPR1_DS {0} ' . format ( dataset_number ), reg3 )], weights = [ 3. / 6. , 2. / 6. , 1. / 6. ] ) ereg . fit ( self . X_train , self . y_train ) dump ( ereg , model_path ) try : self . evaluate_model ( model = ereg , model_path = model_path ) except : print ( 'Prediction is out of the range.' ) def inference_model ( self , model = None , model_path = None ): if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test [ 3 : 4 , ]) print ( 'X_test {0} ' . format ( self . X_test [ 3 : 4 , ])) print ( '------' ) print ( 'y_test or y_true {0} ' . format ( y_true [ 3 : 4 , ])) print ( '------' ) print ( 'y_pred by model {0} ' . format ( y_pred )) y_true , y_pred = self . y_test , model . predict ( self . X_test ) df_new = pd . DataFrame ( columns = self . df . columns ) for i , row in self . y_test . iteritems (): print ( '' , i , row ) df_new = df_new . append ( self . df . loc [ i ], ignore_index = True ) df_new [ 'y_true' ] = self . y_test . values df_new [ 'y_pred' ] = list ( y_pred ) df_new . to_csv ( model_path [: - 7 ] + '_inference_result.csv' , index = True , index_label = 'Row' ) def inference_model2 ( self , model = None , model_path = None , predict_data_path = None ): if model is None : model = joblib . load ( model_path ) df_predict_data = pd . read_csv ( predict_data_path , delimiter = ',' , index_col = False ) X_test1 = df_predict_data . iloc [:, 1 :] X_test = self . scaler . transform ( X_test1 ) y_pred = model . predict ( X_test ) df_new = pd . DataFrame ( df_predict_data . iloc [:, 0 ], columns = [ 'Class' ]) df_new [ 'PredictedTestability' ] = list ( y_pred ) print ( df_new ) def evaluate_model ( self , model = None , model_path = None ): if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test ) # y_score = model.predict_proba(X_test) # Print all classifier model metrics print ( 'Evaluating regressor ...' ) print ( 'Regressor minimum prediction' , min ( y_pred ), 'Regressor maximum prediction' , max ( y_pred )) df = pd . DataFrame () df [ 'r2_score_uniform_average' ] = [ r2_score ( y_true , y_pred , multioutput = 'uniform_average' )] df [ 'r2_score_variance_weighted' ] = [ r2_score ( y_true , y_pred , multioutput = 'variance_weighted' )] df [ 'explained_variance_score_uniform_average' ] = [ explained_variance_score ( y_true , y_pred , multioutput = 'uniform_average' )] df [ 'explained_variance_score_variance_weighted' ] = [ explained_variance_score ( y_true , y_pred , multioutput = 'variance_weighted' )] df [ 'mean_absolute_error' ] = [ mean_absolute_error ( y_true , y_pred )] df [ 'mean_squared_error_MSE' ] = [ mean_squared_error ( y_true , y_pred )] df [ 'mean_squared_error_RMSE' ] = [ mean_squared_error ( y_true , y_pred , squared = False )] df [ 'median_absolute_error' ] = [ median_absolute_error ( y_true , y_pred )] if min ( y_pred ) >= 0 : df [ 'mean_squared_log_error' ] = [ mean_squared_log_error ( y_true , y_pred )] # To handle ValueError: Mean Tweedie deviance error with power=2 can only be used on \\ # strictly positive y and y_pred. if min ( y_pred > 0 ) and min ( y_true ) > 0 : df [ 'mean_poisson_deviance' ] = [ mean_poisson_deviance ( y_true , y_pred , )] df [ 'mean_gamma_deviance' ] = [ mean_gamma_deviance ( y_true , y_pred , )] df [ 'max_error' ] = [ max_error ( y_true , y_pred )] df . to_csv ( model_path [: - 7 ] + '_evaluation_metrics_R1.csv' , index = True , index_label = 'Row' ) def statistical_comparison ( self , ): model1 = load ( r 'results/VR1_DS1.joblib' ) # model2 = load(r'results/RFR1_DS1.joblib') # model2 = load(r'results/MLPR1_DS1.joblib') model2 = load ( r 'results/HGBR1_DS1.joblib' ) y_pred1 = model1 . predict ( self . X_test ) y_pred2 = model2 . predict ( self . X_test ) y_true = np . array ( list ( self . y_test . values )) y_pred1 = np . array ( list ( y_pred1 )) y_pred2 = np . array ( list ( y_pred2 )) output_errors1 = ( abs ( y_true - y_pred1 )) output_errors2 = ( abs ( y_true - y_pred2 )) s , p = ttest_ind ( list ( output_errors1 ), list ( output_errors2 ), alternative = \"less\" , ) print ( f 'statistic = { s } , p-value= { p } ' ) def statistical_comparison2 ( self , ): model1 = load ( r 'results/VR1_DS1.joblib' ) # model2 = load(r'results/RFR1_DS1.joblib') # model2 = load(r'results/MLPR1_DS1.joblib') # model2 = load(r'results/HGBR1_DS1.joblib') # model2 = load(r'results/SGDR1_DS1.joblib') model2 = load ( r 'results/DTR1_DS1.joblib' ) # model2 = load(r'results/DTR1_DS3.joblib') me1 = [] me2 = [] for i in range ( 100 ): # 100 x_test = [] y_test = [] for j in range ( 2000 ): # 2000 random_index = random . randint ( 0 , len ( self . X_test ) - 1 ) x_test . append ( list ( self . X_test )[ random_index ]) y_test . append ( self . y_test . values [ random_index ]) y_pred1 = model1 . predict ( x_test ) y_pred2 = model2 . predict ( x_test ) # me1.append(mean_squared_error(y_test, y_pred1)) # me2.append(mean_squared_error(y_test, y_pred2)) me1 . append ( r2_score ( y_test , y_pred1 )) me2 . append ( r2_score ( y_test , y_pred2 )) # print('me1', me1) # print('me2', me2) s , p = ttest_ind ( me2 , me1 , alternative = \"less\" , ) print ( f 'statistic t-test = { s } , p-value= { p } ' ) s , p = wilcoxon ( me2 , me1 , alternative = \"less\" , ) print ( f 'statistic wilcoxon = { s } , p-value= { p } ' ) s , p = mannwhitneyu ( me2 , me1 , alternative = \"less\" , ) print ( f 'statistic Mann-Whitney U = { s } , p-value= { p } ' ) print () regress ( self , model_path = None , model_number = None ) :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: Source code in adafest\\code\\testability\\ml_models_testability.py def regress ( self , model_path : str = None , model_number : int = None ): \"\"\" :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: \"\"\" regressor = None parameters = None if model_number == 1 : regressor = DecisionTreeRegressor ( random_state = 23 , ) # Set the parameters to be used for tuning by cross-validation parameters = { # 'criterion': ['mse', 'friedman_mse', 'mae'], 'max_depth' : range ( 3 , 50 , 5 ), 'min_samples_split' : range ( 2 , 30 , 2 ) } elif model_number == 2 : regressor = RandomForestRegressor ( random_state = 19 , ) parameters = { 'n_estimators' : range ( 100 , 200 , 100 ), # 'criterion': ['mse', 'mae'], 'max_depth' : range ( 10 , 50 , 10 ), # 'min_samples_split': range(2, 30, 2), # 'max_features': ['auto', 'sqrt', 'log2'] } elif model_number == 3 : regressor = GradientBoostingRegressor ( n_estimators = 400 , learning_rate = 0.05 , random_state = 17 , ) parameters = { # 'loss': ['ls', 'lad', ], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_split' : range ( 2 , 30 , 3 ) } elif model_number == 4 : regressor = HistGradientBoostingRegressor ( max_iter = 400 , learning_rate = 0.05 , random_state = 13 , ) parameters = { # 'loss': ['least_squares', 'least_absolute_deviation'], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_leaf' : range ( 5 , 50 , 10 ) } elif model_number == 5 : regressor = linear_model . SGDRegressor ( early_stopping = True , n_iter_no_change = 5 , random_state = 11 , ) parameters = { 'loss' : [ 'squared_loss' , 'huber' , 'epsilon_insensitive' ], 'penalty' : [ 'l2' , 'l1' , 'elasticnet' ], 'max_iter' : range ( 50 , 1000 , 50 ), 'learning_rate' : [ 'invscaling' , 'optimal' , 'constant' , 'adaptive' ], 'eta0' : [ 0.1 , 0.01 ], 'average' : [ 32 , ] } elif model_number == 6 : regressor = MLPRegressor ( random_state = 7 , ) parameters = { 'hidden_layer_sizes' : [( 256 , 100 ), ( 512 , 256 , 100 ), ], 'activation' : [ 'tanh' , ], 'solver' : [ 'adam' , ], 'max_iter' : range ( 50 , 200 , 50 ) } if regressor is None : return if parameters is None : return # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.20 , random_state = 101 ) # Find the best model using gird-search with cross-validation clf = GridSearchCV ( regressor , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 7 , refit = 'neg_root_mean_squared_error' ) print ( 'fitting model number' , model_number ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # self.evaluate_model_class(model=clf, model_path=model_path) # self.inference_model(model=clf, model_path=model_path) print ( '=' * 75 )","title":"Testability prediction"},{"location":"modules/testability_prediction/#testability-prediction","text":"The replication package for the paper 'An ensemble meta-estimator to predict source code testability' published in Applied Soft Computing Journal.","title":"Testability prediction"},{"location":"modules/testability_prediction/#adafest.code.testability.ml_models_testability--goal","text":"This script implements machine learning models for predicting testability presented in Applied Soft Computing Journal","title":"Goal"},{"location":"modules/testability_prediction/#adafest.code.testability.ml_models_testability--machine-learning-models","text":"Model 1: DecisionTreeRegressor Model 2: RandomForestRegressor Model 3: GradientBoostingRegressor Model 4: HistGradientBoostingRegressor Model 5: SGDRegressor Model 6: MLPRegressor","title":"Machine learning models"},{"location":"modules/testability_prediction/#adafest.code.testability.ml_models_testability--learning-datasets","text":"Dataset: Applied preprocessing: Number of metrics DS1: (default) Simple classes elimination, data classes elimination, outliers elimination, and metric standardization: 262 features DS2: DS1 + Feature selection: 20 features DS3: DS1 + Context vector elimination: 194 features DS4: DS1 + Context vector elimination and lexical metrics elimination 177 DS5: DS1 + Systematically generated metrics elimination 71 DS6: Top 15 important source code metrics affecting testability","title":"Learning datasets"},{"location":"modules/testability_prediction/#adafest.code.testability.ml_models_testability--model-dependent-variable","text":"Testability of class X: T(X) = E[C]/ (1 + omega) ^ (|n| - 1) where E[C] = 1/3 StatementCoverage + 1/3 BranchCoverage + 1/3*MutationCoverage","title":"Model dependent variable"},{"location":"modules/testability_prediction/#adafest.code.testability.ml_models_testability--results","text":"The results will be saved in ../../results directory","title":"Results"},{"location":"modules/testability_prediction/#adafest.code.testability.ml_models_testability--inferences","text":"Use the method inference_model2 of the class Regression to predict testability of new Java classes.","title":"Inferences"},{"location":"modules/testability_prediction/#adafest.code.testability.ml_models_testability.Regression","text":"Source code in adafest\\code\\testability\\ml_models_testability.py class Regression : def __init__ ( self , df_path : str = None , feature_selection_mode = False ): self . df = pd . read_csv ( df_path , delimiter = ',' , index_col = False ) self . X_train1 , self . X_test1 , self . y_train , self . y_test = train_test_split ( self . df . iloc [:, 1 : - 1 ], self . df . iloc [:, - 1 ], test_size = 0.25 , random_state = 117 , ) # -- Feature selection (For DS2) if feature_selection_mode : selector = feature_selection . SelectKBest ( feature_selection . f_regression , k = 20 ) # clf = linear_model.LassoCV(eps=1e-3, n_alphas=100, normalize=True, max_iter=5000, tol=1e-4) # clf.fit(self.X_train1, self.y_train) # importance = np.abs(clf.coef_) # print('importance', importance) # clf = RandomForestRegressor() # selector = feature_selection.SelectFromModel(clf, prefit=False, norm_order=2, max_features=20,) selector . fit ( self . X_train1 , self . y_train ) # Get columns to keep and create new dataframe with only selected features cols = selector . get_support ( indices = True ) self . X_train1 = self . X_train1 . iloc [:, cols ] self . X_test1 = self . X_test1 . iloc [:, cols ] print ( 'Selected columns by feature selection:' , self . X_train1 . columns ) # quit() # --- End of feature selection # Standardization # self.scaler = preprocessing.RobustScaler(with_centering=True, with_scaling=True, unit_variance=True) # self.scaler = preprocessing.StandardScaler() self . scaler = QuantileTransformer ( n_quantiles = 1000 , random_state = 11 ) self . scaler . fit ( self . X_train1 ) self . X_train = self . scaler . transform ( self . X_train1 ) self . X_test = self . scaler . transform ( self . X_test1 ) dump ( self . scaler , df_path [: - 4 ] + '_scaler.joblib' ) # quit() def regress ( self , model_path : str = None , model_number : int = None ): \"\"\" :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: \"\"\" regressor = None parameters = None if model_number == 1 : regressor = DecisionTreeRegressor ( random_state = 23 , ) # Set the parameters to be used for tuning by cross-validation parameters = { # 'criterion': ['mse', 'friedman_mse', 'mae'], 'max_depth' : range ( 3 , 50 , 5 ), 'min_samples_split' : range ( 2 , 30 , 2 ) } elif model_number == 2 : regressor = RandomForestRegressor ( random_state = 19 , ) parameters = { 'n_estimators' : range ( 100 , 200 , 100 ), # 'criterion': ['mse', 'mae'], 'max_depth' : range ( 10 , 50 , 10 ), # 'min_samples_split': range(2, 30, 2), # 'max_features': ['auto', 'sqrt', 'log2'] } elif model_number == 3 : regressor = GradientBoostingRegressor ( n_estimators = 400 , learning_rate = 0.05 , random_state = 17 , ) parameters = { # 'loss': ['ls', 'lad', ], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_split' : range ( 2 , 30 , 3 ) } elif model_number == 4 : regressor = HistGradientBoostingRegressor ( max_iter = 400 , learning_rate = 0.05 , random_state = 13 , ) parameters = { # 'loss': ['least_squares', 'least_absolute_deviation'], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_leaf' : range ( 5 , 50 , 10 ) } elif model_number == 5 : regressor = linear_model . SGDRegressor ( early_stopping = True , n_iter_no_change = 5 , random_state = 11 , ) parameters = { 'loss' : [ 'squared_loss' , 'huber' , 'epsilon_insensitive' ], 'penalty' : [ 'l2' , 'l1' , 'elasticnet' ], 'max_iter' : range ( 50 , 1000 , 50 ), 'learning_rate' : [ 'invscaling' , 'optimal' , 'constant' , 'adaptive' ], 'eta0' : [ 0.1 , 0.01 ], 'average' : [ 32 , ] } elif model_number == 6 : regressor = MLPRegressor ( random_state = 7 , ) parameters = { 'hidden_layer_sizes' : [( 256 , 100 ), ( 512 , 256 , 100 ), ], 'activation' : [ 'tanh' , ], 'solver' : [ 'adam' , ], 'max_iter' : range ( 50 , 200 , 50 ) } if regressor is None : return if parameters is None : return # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.20 , random_state = 101 ) # Find the best model using gird-search with cross-validation clf = GridSearchCV ( regressor , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 7 , refit = 'neg_root_mean_squared_error' ) print ( 'fitting model number' , model_number ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # self.evaluate_model_class(model=clf, model_path=model_path) # self.inference_model(model=clf, model_path=model_path) print ( '=' * 75 ) def vote ( self , model_path = None , dataset_number = 1 ): # Trained regressors reg1 = load ( r '../../results/HGBR1_DS {0} .joblib' . format ( dataset_number )) reg2 = load ( r '../../results/RFR1_DS {0} .joblib' . format ( dataset_number )) reg3 = load ( r '../../results/MLPR1_DS {0} .joblib' . format ( dataset_number )) # reg4 = load(r'results/SGDR1_DS1.joblib') ereg = VotingRegressor ( [( 'HGBR1_DS {0} ' . format ( dataset_number ), reg1 ), ( 'RFR1_DS {0} ' . format ( dataset_number ), reg2 ), ( 'MLPR1_DS {0} ' . format ( dataset_number ), reg3 )], weights = [ 3. / 6. , 2. / 6. , 1. / 6. ] ) ereg . fit ( self . X_train , self . y_train ) dump ( ereg , model_path ) try : self . evaluate_model ( model = ereg , model_path = model_path ) except : print ( 'Prediction is out of the range.' ) def inference_model ( self , model = None , model_path = None ): if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test [ 3 : 4 , ]) print ( 'X_test {0} ' . format ( self . X_test [ 3 : 4 , ])) print ( '------' ) print ( 'y_test or y_true {0} ' . format ( y_true [ 3 : 4 , ])) print ( '------' ) print ( 'y_pred by model {0} ' . format ( y_pred )) y_true , y_pred = self . y_test , model . predict ( self . X_test ) df_new = pd . DataFrame ( columns = self . df . columns ) for i , row in self . y_test . iteritems (): print ( '' , i , row ) df_new = df_new . append ( self . df . loc [ i ], ignore_index = True ) df_new [ 'y_true' ] = self . y_test . values df_new [ 'y_pred' ] = list ( y_pred ) df_new . to_csv ( model_path [: - 7 ] + '_inference_result.csv' , index = True , index_label = 'Row' ) def inference_model2 ( self , model = None , model_path = None , predict_data_path = None ): if model is None : model = joblib . load ( model_path ) df_predict_data = pd . read_csv ( predict_data_path , delimiter = ',' , index_col = False ) X_test1 = df_predict_data . iloc [:, 1 :] X_test = self . scaler . transform ( X_test1 ) y_pred = model . predict ( X_test ) df_new = pd . DataFrame ( df_predict_data . iloc [:, 0 ], columns = [ 'Class' ]) df_new [ 'PredictedTestability' ] = list ( y_pred ) print ( df_new ) def evaluate_model ( self , model = None , model_path = None ): if model is None : model = joblib . load ( model_path ) y_true , y_pred = self . y_test , model . predict ( self . X_test ) # y_score = model.predict_proba(X_test) # Print all classifier model metrics print ( 'Evaluating regressor ...' ) print ( 'Regressor minimum prediction' , min ( y_pred ), 'Regressor maximum prediction' , max ( y_pred )) df = pd . DataFrame () df [ 'r2_score_uniform_average' ] = [ r2_score ( y_true , y_pred , multioutput = 'uniform_average' )] df [ 'r2_score_variance_weighted' ] = [ r2_score ( y_true , y_pred , multioutput = 'variance_weighted' )] df [ 'explained_variance_score_uniform_average' ] = [ explained_variance_score ( y_true , y_pred , multioutput = 'uniform_average' )] df [ 'explained_variance_score_variance_weighted' ] = [ explained_variance_score ( y_true , y_pred , multioutput = 'variance_weighted' )] df [ 'mean_absolute_error' ] = [ mean_absolute_error ( y_true , y_pred )] df [ 'mean_squared_error_MSE' ] = [ mean_squared_error ( y_true , y_pred )] df [ 'mean_squared_error_RMSE' ] = [ mean_squared_error ( y_true , y_pred , squared = False )] df [ 'median_absolute_error' ] = [ median_absolute_error ( y_true , y_pred )] if min ( y_pred ) >= 0 : df [ 'mean_squared_log_error' ] = [ mean_squared_log_error ( y_true , y_pred )] # To handle ValueError: Mean Tweedie deviance error with power=2 can only be used on \\ # strictly positive y and y_pred. if min ( y_pred > 0 ) and min ( y_true ) > 0 : df [ 'mean_poisson_deviance' ] = [ mean_poisson_deviance ( y_true , y_pred , )] df [ 'mean_gamma_deviance' ] = [ mean_gamma_deviance ( y_true , y_pred , )] df [ 'max_error' ] = [ max_error ( y_true , y_pred )] df . to_csv ( model_path [: - 7 ] + '_evaluation_metrics_R1.csv' , index = True , index_label = 'Row' ) def statistical_comparison ( self , ): model1 = load ( r 'results/VR1_DS1.joblib' ) # model2 = load(r'results/RFR1_DS1.joblib') # model2 = load(r'results/MLPR1_DS1.joblib') model2 = load ( r 'results/HGBR1_DS1.joblib' ) y_pred1 = model1 . predict ( self . X_test ) y_pred2 = model2 . predict ( self . X_test ) y_true = np . array ( list ( self . y_test . values )) y_pred1 = np . array ( list ( y_pred1 )) y_pred2 = np . array ( list ( y_pred2 )) output_errors1 = ( abs ( y_true - y_pred1 )) output_errors2 = ( abs ( y_true - y_pred2 )) s , p = ttest_ind ( list ( output_errors1 ), list ( output_errors2 ), alternative = \"less\" , ) print ( f 'statistic = { s } , p-value= { p } ' ) def statistical_comparison2 ( self , ): model1 = load ( r 'results/VR1_DS1.joblib' ) # model2 = load(r'results/RFR1_DS1.joblib') # model2 = load(r'results/MLPR1_DS1.joblib') # model2 = load(r'results/HGBR1_DS1.joblib') # model2 = load(r'results/SGDR1_DS1.joblib') model2 = load ( r 'results/DTR1_DS1.joblib' ) # model2 = load(r'results/DTR1_DS3.joblib') me1 = [] me2 = [] for i in range ( 100 ): # 100 x_test = [] y_test = [] for j in range ( 2000 ): # 2000 random_index = random . randint ( 0 , len ( self . X_test ) - 1 ) x_test . append ( list ( self . X_test )[ random_index ]) y_test . append ( self . y_test . values [ random_index ]) y_pred1 = model1 . predict ( x_test ) y_pred2 = model2 . predict ( x_test ) # me1.append(mean_squared_error(y_test, y_pred1)) # me2.append(mean_squared_error(y_test, y_pred2)) me1 . append ( r2_score ( y_test , y_pred1 )) me2 . append ( r2_score ( y_test , y_pred2 )) # print('me1', me1) # print('me2', me2) s , p = ttest_ind ( me2 , me1 , alternative = \"less\" , ) print ( f 'statistic t-test = { s } , p-value= { p } ' ) s , p = wilcoxon ( me2 , me1 , alternative = \"less\" , ) print ( f 'statistic wilcoxon = { s } , p-value= { p } ' ) s , p = mannwhitneyu ( me2 , me1 , alternative = \"less\" , ) print ( f 'statistic Mann-Whitney U = { s } , p-value= { p } ' ) print ()","title":"Regression"},{"location":"modules/testability_prediction/#adafest.code.testability.ml_models_testability.Regression.regress","text":":param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: Source code in adafest\\code\\testability\\ml_models_testability.py def regress ( self , model_path : str = None , model_number : int = None ): \"\"\" :param model_path: :param model_number: 1: DTR, 2: RFR, 3: GBR, 4: HGBR, 5: SGDR, 6: MLPR, :return: \"\"\" regressor = None parameters = None if model_number == 1 : regressor = DecisionTreeRegressor ( random_state = 23 , ) # Set the parameters to be used for tuning by cross-validation parameters = { # 'criterion': ['mse', 'friedman_mse', 'mae'], 'max_depth' : range ( 3 , 50 , 5 ), 'min_samples_split' : range ( 2 , 30 , 2 ) } elif model_number == 2 : regressor = RandomForestRegressor ( random_state = 19 , ) parameters = { 'n_estimators' : range ( 100 , 200 , 100 ), # 'criterion': ['mse', 'mae'], 'max_depth' : range ( 10 , 50 , 10 ), # 'min_samples_split': range(2, 30, 2), # 'max_features': ['auto', 'sqrt', 'log2'] } elif model_number == 3 : regressor = GradientBoostingRegressor ( n_estimators = 400 , learning_rate = 0.05 , random_state = 17 , ) parameters = { # 'loss': ['ls', 'lad', ], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_split' : range ( 2 , 30 , 3 ) } elif model_number == 4 : regressor = HistGradientBoostingRegressor ( max_iter = 400 , learning_rate = 0.05 , random_state = 13 , ) parameters = { # 'loss': ['least_squares', 'least_absolute_deviation'], 'max_depth' : range ( 10 , 50 , 10 ), 'min_samples_leaf' : range ( 5 , 50 , 10 ) } elif model_number == 5 : regressor = linear_model . SGDRegressor ( early_stopping = True , n_iter_no_change = 5 , random_state = 11 , ) parameters = { 'loss' : [ 'squared_loss' , 'huber' , 'epsilon_insensitive' ], 'penalty' : [ 'l2' , 'l1' , 'elasticnet' ], 'max_iter' : range ( 50 , 1000 , 50 ), 'learning_rate' : [ 'invscaling' , 'optimal' , 'constant' , 'adaptive' ], 'eta0' : [ 0.1 , 0.01 ], 'average' : [ 32 , ] } elif model_number == 6 : regressor = MLPRegressor ( random_state = 7 , ) parameters = { 'hidden_layer_sizes' : [( 256 , 100 ), ( 512 , 256 , 100 ), ], 'activation' : [ 'tanh' , ], 'solver' : [ 'adam' , ], 'max_iter' : range ( 50 , 200 , 50 ) } if regressor is None : return if parameters is None : return # Set the objectives which must be optimized during parameter tuning # scoring = ['r2', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'neg_mean_absolute_error',] scoring = [ 'neg_root_mean_squared_error' , ] # CrossValidation iterator object: # https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html cv = ShuffleSplit ( n_splits = 5 , test_size = 0.20 , random_state = 101 ) # Find the best model using gird-search with cross-validation clf = GridSearchCV ( regressor , param_grid = parameters , scoring = scoring , cv = cv , n_jobs = 7 , refit = 'neg_root_mean_squared_error' ) print ( 'fitting model number' , model_number ) clf . fit ( X = self . X_train , y = self . y_train ) print ( 'Writing grid search result ...' ) df = pd . DataFrame ( clf . cv_results_ , ) df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results.csv' , index = False ) df = pd . DataFrame () print ( 'Best parameters set found on development set:' , clf . best_params_ ) df [ 'best_parameters_development_set' ] = [ clf . best_params_ ] print ( 'Best classifier score on development set:' , clf . best_score_ ) df [ 'best_score_development_set' ] = [ clf . best_score_ ] print ( 'best classifier score on test set:' , clf . score ( self . X_test , self . y_test )) df [ 'best_score_test_set:' ] = [ clf . score ( self . X_test , self . y_test )] df . to_csv ( model_path [: - 7 ] + '_grid_search_cv_results_best.csv' , index = False ) # Save and evaluate the best obtained model print ( 'Writing evaluation result ...' ) clf = clf . best_estimator_ y_true , y_pred = self . y_test , clf . predict ( self . X_test ) dump ( clf , model_path ) self . evaluate_model ( model = clf , model_path = model_path ) # self.evaluate_model_class(model=clf, model_path=model_path) # self.inference_model(model=clf, model_path=model_path) print ( '=' * 75 )","title":"regress()"}]}